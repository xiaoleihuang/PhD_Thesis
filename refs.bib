@inproceedings{huang2018modeling,
    abstract = {Categorizing patient{\{}'{\}}s intentions in conversational assessment can help decision making in clinical treatments. Many conversation corpora span broaden a series of time stages. However, it is not clear that how the themes shift in the conversation impact on the performance of human intention categorization (eg., patients might show different behaviors during the beginning versus the end). This paper proposes a method that models the temporal factor by using domain adaptation on clinical dialogue corpora, Motivational Interviewing (MI). We deploy Bi-LSTM and topic model jointly to learn language usage change across different time sessions. We conduct experiments on the MI corpora to show the promising improvement after considering temporality in the classification task.},
    author = {Huang, Xiaolei and Liu, Lixing and Carey, Kate and Woolley, Joshua and Scherer, Stefan and Borsari, Brian},
    booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
    doi = {10.18653/v1/d18-1074},
    pages = {696--701},
    title = {{Modeling Temporality of Human Intentions by Domain Adaptation}},
    year = {2019},
    address = {Brussels, Belgium},
}

@book{chase1992america,
    abstract = {An extensively revised version of the chronological history of American music first published in 1955 (New York: McGraw-Hill), cited as RILM [ref]1955-00631[/ref]. Other editions are cited as RILM [ref]1966-01454[/ref], [ref]1971-03553[/ref], and [ref]1981-04414[/ref]. A Chinese translation of the book is abstracted as RILM [ref]1971-04977[/ref].},
    author = {Kroeger, Karl and Chase, Gilbert},
    booktitle = {Notes},
    doi = {10.2307/941969},
    issn = {00274380},
    pages = {381},
    publisher = {University of Illinois Press},
    title = {{America's Music: From the Pilgrims to the Present}},
    volume = {47},
    year = {1990}
}

@inproceedings{camacho2018role,
    title = "On the Role of Text Preprocessing in Neural Network Architectures: An Evaluation Study on Text Categorization and Sentiment Analysis",
    author = "Camacho-Collados, Jose  and
      Pilehvar, Mohammad Taher",
    booktitle = "Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP}",
    month = nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-5406",
    doi = "10.18653/v1/W18-5406",
    pages = "40--46",
    abstract = "Text preprocessing is often the first step in the pipeline of a Natural Language Processing (NLP) system, with potential impact in its final performance. Despite its importance, text preprocessing has not received much attention in the deep learning literature. In this paper we investigate the impact of simple text preprocessing decisions (particularly tokenizing, lemmatizing, lowercasing and multiword grouping) on the performance of a standard neural text classifier. We perform an extensive evaluation on standard benchmarks from text categorization and sentiment analysis. While our experiments show that a simple tokenization of input text is generally adequate, they also highlight significant degrees of variability across preprocessing techniques. This reveals the importance of paying attention to this usually-overlooked step in the pipeline, particularly when comparing different models. Finally, our evaluation provides insights into the best preprocessing practices for training word embeddings.",
}

@inproceedings{Rao2011,
    abstract = {We present several novel minimally-supervised models for detecting latent attributes of social media users, with a focus on ethnicity and gender. Previous work on ethnicity detection has used coarse-grained widely separated classes of ethnicity and assumed the existence of large amounts of training data such as the US census, simplifying the problem. Instead, we examine content generated by users in addition to name morphophonemics to detect ethnicity and gender. Further, we address this problem in a challenging setting where the ethnicity classes are more fine grained – ethnicity classes in Nigeria – and with very limited training data.},
    author = {Rao, D. and Paul, M. and Fink, C. and Yarowsky, D. and Oates, T. and Coppersmith, G.},
    booktitle = {Fifth International AAAI Conference on Weblogs and Social Media},
    keywords = {Poster Papers},
    pages = {598--601},
    title = {{Hierarchical Bayesian Models for Latent Attribute Detection in Social Media}},
    url = {http://www.aaai.org/ocs/index.php/ICWSM/ICWSM11/paper/download/2881/3229},
    year = {2011}
}

@article{madson2009training,
    abstract = {Motivational interviewing (MI), an evidence-based counseling approach, has received much recognition from a wide variety of health care professionals. Because of the rising interest in MI, there is increasing demand for training in this counseling approach. The MI training community has answered this call and as a result placed much emphasis on studying the MI training process. The purpose of this article is to provide a systematic review of the published research on MI training. Our goal is to provide a consolidated account of MI trainings outlining the populations receiving training, methods used, and training outcomes. We also identify which aspects of the (W. R. Miller {\&} T. B. Moyers, 2006) eight stages of learning MI each study addressed. Recommendations for advancing the MI training research are highlighted. {\textcopyright} 2009 Elsevier Inc. All rights reserved.},
    author = {Madson, Michael B. and Loignon, Andrew C. and Lane, Claire},
    doi = {10.1016/j.jsat.2008.05.005},
    issn = {07405472},
    journal = {Journal of Substance Abuse Treatment},
    keywords = {Education,Motivational interviewing,Training},
    number = {1},
    pages = {101--109},
    publisher = {Elsevier},
    title = {{Training in motivational interviewing: A systematic review}},
    volume = {36},
    year = {2009}
}

@article{wagner2012age,
    abstract = {Distinguishing linguistic change at the community level (‘generational change') from linguistic change at the individual level (‘age grading') is ‘‘one of the major issues in contemporary sociolin- guistics'' (Tagliamonte 2012:247). This article gives a brief history of the study of language change in the community, before turning to the types of linguistic behavior that have been observed across individuals' lifespans. The article also discusses the meanings that have been attributed to the term ‘age grading', arguing that consensus cannot be reached without more longitudinal work to determine the limits of lifespan linguistic change.},
    author = {Wagner, Suzanne Evans},
    doi = {10.1002/lnc3.343},
    issn = {1749818X},
    journal = {Linguistics and Language Compass},
    number = {6},
    pages = {371--382},
    publisher = {Wiley Online Library},
    title = {{Age Grading in Sociolinguistic Theory}},
    volume = {6},
    year = {2012}
}

@inproceedings{elazar2018adversarial,
    abstract = {Recent advances in Representation Learning and Adversarial Training seem to succeed in removing unwanted features from the learned representation. We show that demographic information of authors is encoded in -- and can be recovered from -- the intermediate representations learned by text-based neural classifiers. The implication is that decisions of classifiers trained on textual data are not agnostic to -- and likely condition on -- demographic attributes. When attempting to remove such demographic information using adversarial training, we find that while the adversarial component achieves chance-level development-set accuracy during training, a post-hoc classifier, trained on the encoded sentences from the first part, still manages to reach substantially higher classification accuracies on the same data. This behavior is consistent across several tasks, demographic properties and datasets. We explore several techniques to improve the effectiveness of the adversarial component. Our main conclusion is a cautionary one: do not rely on the adversarial training to achieve invariant representation to sensitive features.},
    author = {Elazar, Yanai and Goldberg, Yoav},
    booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
    doi = {10.18653/v1/d18-1002},
    pages = {11--21},
    title = {{Adversarial Removal of Demographic Attributes from Text Data}},
    year = {2019}
}

@article{apodaca2014sustain,
    abstract = {{\textcopyright} 2014 American Psychological Association. Within-session client language that represents a movement toward behavior change (change talk) has been linked to better treatment outcomes in the literature on motivational interviewing (MI). There has been somewhat less study of the impact of client language against change (sustain talk) on outcomes following an MI session. This study examined the role of both client change talk and sustain talk, as well as therapist language, occurring during a brief motivational intervention (BMI) session with college students who had violated college alcohol policy (N = 92). Audiotapes of these sessions were coded using a therapy process coding system. A series of hierarchical regressions were used to examine the relationships among therapist MI-consistent and MI-inconsistent language, client change talk and sustain talk, as well as global measures of relational variables, and drinking outcomes. Contrary to prior research, sustain talk, but not change talk, predicted poorer alcohol use outcomes following the BMI at 3-and 12-month follow-up assessments. Higher levels of client self-exploration during the session also predicted improved drinking outcomes. Therapist measures of MI-consistent and MI-inconsistent language, and global measures of therapist acceptance and MI spirit were unrelated to client drinking outcomes. Results suggest that client sustain talk and self-exploration during the session play an important role in determining drinking outcomes among mandated college students receiving a BMI addressing alcohol use.},
    author = {Apodaca, Timothy R. and Borsari, Brian and Jackson, Kristina M. and Magill, Molly and Longabaugh, Richard and Mastroleo, Nadine R. and Barnett, Nancy P.},
    doi = {10.1037/a0037296},
    issn = {19391501},
    journal = {Psychology of Addictive Behaviors},
    keywords = {Alcohol use,Brief intervention,Change language,Motivational interviewing,Therapy process},
    number = {3},
    pages = {631--638},
    publisher = {American Psychological Association},
    title = {{Sustain talk predicts poorer outcomes among mandated college student drinkers receiving a brief motivational intervention}},
    volume = {28},
    year = {2014}
}

@inproceedings{gibson2017attention,
    author = {Gibson, James and Can, Doǧan and Georgiou, Panayiotis and Atkins, David C. and Narayanan, Shrikanth},
    booktitle = {Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
    doi = {10.21437/Interspeech.2017-218},
    issn = {19909772},
    keywords = {Attention,Behavioral signal processing,Motivational interviews,Recurrent neural networks,Word embedding},
    pages = {3251--3255},
    title = {{Attention networks for modeling behaviors in addiction counseling}},
    volume = {2017-Augus},
    year = {2017}
}

@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in neural information processing systems},
  pages={5998--6008},
  year={2017}
}

@inproceedings{lynn2017human,
    title = "Human Centered {NLP} with User-Factor Adaptation",
    author = "Lynn, Veronica  and
      Son, Youngseo  and
      Kulkarni, Vivek  and
      Balasubramanian, Niranjan  and
      Schwartz, H. Andrew",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D17-1119",
    doi = "10.18653/v1/D17-1119",
    pages = "1146--1155",
    abstract = "We pose the general task of user-factor adaptation {--} adapting supervised learning models to real-valued user factors inferred from a background of their language, reflecting the idea that a piece of text should be understood within the context of the user that wrote it. We introduce a continuous adaptation technique, suited for real-valued user factors that are common in social science and bringing us closer to personalized NLP, adapting to each user uniquely. We apply this technique with known user factors including age, gender, and personality traits, as well as latent factors, evaluating over five tasks: POS tagging, PP-attachment, sentiment analysis, sarcasm detection, and stance detection. Adaptation provides statistically significant benefits for 3 of the 5 tasks: up to +1.2 points for PP-attachment, +3.4 points for sarcasm, and +3.0 points for stance.",
}

@inproceedings{szymanski2017temporal,
    title = "Temporal Word Analogies: Identifying Lexical Replacement with Diachronic Word Embeddings",
    author = "Szymanski, Terrence",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    doi = "10.18653/v1/P17-2071",
    pages = "448--453",
    abstract = "This paper introduces the concept of temporal word analogies: pairs of words which occupy the same semantic space at different points in time. One well-known property of word embeddings is that they are able to effectively model traditional word analogies ({``}word $w_1$ is to word $w_2$ as word $w_3$ is to word $w_4${''}) through vector addition. Here, I show that temporal word analogies ({``}word $w_1$ at time $t_\alpha$ is like word $w_2$ at time $t_\beta${''}) can effectively be modeled with diachronic word embeddings, provided that the independent embedding spaces from each time period are appropriately transformed into a common vector space. When applied to a diachronic corpus of news articles, this method is able to identify temporal word analogies such as {``}Ronald Reagan in 1987 is like Bill Clinton in 1997{''}, or {``}Walkman in 1987 is like iPod in 2007{''}.",
}

@article{romano2016understanding,
    abstract = {Abstract Objective: The current study systematically reviews evidence for a causal chain model suggested by Miller and Rose to account for the efficacy of Motivational Interviewing (MI). Method: Literature searches were conducted to identify studies delivering MI in an individual format to treat various problem areas. Results: Thirty-seven studies met inclusion criteria. The results suggest that when clinicians utilise MI consistent behaviours, clients are more likely to express language in favour of change. Furthermore, this client language was consistently related to positive client outcome across studies. Conclusions: While the results support some parts of the Miller and Rose model, additional research is needed to confirm the findings in diverse populations. Understanding the mechanisms of MI's effectiveness may maximise the implementation of MI, potentially contributing to better client outcomes.},
    author = {Romano, Mia and Peters, Lorna},
    doi = {10.1080/10503307.2014.954154},
    issn = {14684381},
    journal = {Psychotherapy Research},
    keywords = {behaviour change,change talk,mechanisms,motivational interviewing,review,therapist behaviours},
    number = {2},
    pages = {220--240},
    publisher = {Taylor {\&} Francis},
    title = {{Understanding the process of motivational interviewing: A review of the relational and technical hypotheses}},
    volume = {26},
    year = {2016}
}

@article{chouldechova2018frontiers,
    abstract = {Foreword Rapid advances in machine learning—the form of artiicial intelligence that al-lows computer systems to learn from data—are capturing scientiic, economic, and public interest. Recent years have seen machine learning systems enter everyday usage, while further applications across healthcare, transportation, nance, and more appear set to shape the development of these elds over the coming decades. The societal and economic opportunities that follow these ad-vances are signiicant, and nations are grappling with how artiicial intelligence might aaect society. There are emerging policy debates in the United States and the United Kingdom about how and where society can make best use of machine learning. As the capabilities of machine learning systems and the range of their applica-tions continue to grow, it is therefore particularly timely for the National Acad-emy of Sciences and the Royal Society to bring together leading gures in these elds. Since 2008, the Raymond and Beverly Sackler U.S.-U.K. Scientiic Fo-rum has brought together thought leaders from a variety of scientiic elds to exchange ideas on topics of international scientiic concern and to help forge an enduring partnership between scientists in the United States and the United Kingdom. The forum on " The Frontiers of Machine Learning " took place in the United States on January 31 and February 1, 2017, at the National Academy of Sciences in Washington, D.C. This event brought together leading researchers and policy experts to explore the cutting edges of machine learning research and the implications of technological advances in this eld. This report sum-marizes the high-level discussions at the event focusing on some of the exciting areas of progress in machine learning and the societal debates that follow. The National Academy of Sciences and the Royal Society share a mission to promote the use of science to beneet society and to inform important policy debates. As Presidents of the National Academy of Sciences and the Royal Soci-ety, we are pleased to introduce the latest piece of work supported through the inspired generosity of the Raymond and Beverly Sackler Foundation.},
    author = {Chouldechova, Alexandra and Roth, Aaron},
    doi = {10.17226/25021},
    journal = {The Frontiers of Machine Learning},
    title = {{The Frontiers of Machine Learning}},
    url = {https://arxiv.org/pdf/1810.08810.pdf},
    year = {2018}
}

@inproceedings{wing2011simple,
    abstract = {We investigate automatic geolocation (i.e. identification of the location, expressed as latitude/longitude coordinates) of documents. Geolocation can be an effectivemeans of summarizing large document collections and it is an important component of geographic information retrieval. We describe several simple supervised methods for document geolocation using only the document's raw text as evidence. All of our methods predict locations in the context of geodesic grids of varying degrees of resolution. We evaluate the methods on geotagged Wikipedia articles and Twitter feeds. For Wikipedia, our best method obtains a median prediction error of just 11.8 kilometers. Twitter geolocation is more challenging: we obtain a median error of 479 km, an improvement on previous results for the dataset. {\textcopyright} 2011 Association for Computational Linguistics.},
    author = {Wing, Benjamin P. and Baldridge, Jason},
    booktitle = {ACL-HLT 2011 - Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},
    isbn = {9781932432879},
    pages = {955--964},
    title = {{Simple supervised document geolocation with geodesic grids}},
    volume = {1},
    year = {2011}
}

@inproceedings{blitzer2006domain,
    abstract = {Discriminative learning methods are widely used in natural language process- ing. These methods work best when their training and test data are drawn from the same distribution. For many NLP tasks, however, we are confronted with new domains in which labeled data is scarce or non-existent. In such cases, we seek to adapt existing models from a resource- rich source domain to a resource-poor target domain. We introduce structural correspondence learning to automatically induce correspondences among features from different domains. We test our tech- nique on part of speech tagging and show performance gains for varying amounts of source and target training data, as well as improvements in target domain parsing accuracy using our improved tagger.},
    author = {Blitzer, John and McDonald, Ryan and Pereira, Fernando},
    booktitle = {COLING/ACL 2006 - EMNLP 2006: 2006 Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference},
    isbn = {1932432736},
    organization = {Association for Computational Linguistics},
    pages = {120--128},
    title = {{Domain adaptation with structural correspondence learning}},
    year = {2006}
}

@inproceedings{joshi2013s,
    title = "What{'}s in a Domain? Multi-Domain Learning for Multi-Attribute Data",
    author = "Joshi, Mahesh  and
      Dredze, Mark  and
      Cohen, William W.  and
      Ros{\'e}, Carolyn P.",
    booktitle = "Proceedings of the 2013 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2013",
    address = "Atlanta, Georgia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N13-1080",
    pages = "685--690",
}

@inproceedings{davidson2019racial,
    title = "Racial Bias in Hate Speech and Abusive Language Detection Datasets",
    author = "Davidson, Thomas  and
      Bhattacharya, Debasmita  and
      Weber, Ingmar",
    booktitle = "Proceedings of the Third Workshop on Abusive Language Online",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    doi = "10.18653/v1/W19-3504",
    pages = "25--35",
    abstract = "Technologies for abusive language detection are being developed and applied with little consideration of their potential biases. We examine racial bias in five different sets of Twitter data annotated for hate speech and abusive language. We train classifiers on these datasets and compare the predictions of these classifiers on tweets written in African-American English with those written in Standard American English. The results show evidence of systematic racial bias in all datasets, as classifiers trained on them tend to predict that tweets written in African-American English are abusive at substantially higher rates. If these abusive language detection systems are used in the field they will therefore have a disproportionate negative impact on African-American social media users. Consequently, these systems may discriminate against the groups who are often the targets of the abuse we are trying to detect.",
}

@inproceedings{maier2014language,
    title = "Language variety identification in {S}panish tweets",
    author = "Maier, Wolfgang  and
      G{\'o}mez-Rodr{\'\i}guez, Carlos",
    booktitle = "Proceedings of the {EMNLP}{'}2014 Workshop on Language Technology for Closely Related Languages and Language Variants",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "ACL",
    url = "https://www.aclweb.org/anthology/W14-4204",
    doi = "10.3115/v1/W14-4204",
    pages = "25--35",
}

@article{sun2019fine,
  title={How to Fine-Tune BERT for Text Classification?},
  author={Sun, Chi and Qiu, Xipeng and Xu, Yige and Huang, Xuanjing},
  journal={arXiv preprint arXiv:1905.05583},
  year={2019}
}

@article{Wolf2019HuggingFacesTS,
  title={HuggingFace's Transformers: State-of-the-art Natural Language Processing},
  author={Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and R'emi Louf and Morgan Funtowicz and Jamie Brew},
  journal={ArXiv},
  year={2019},
  volume={abs/1910.03771}
}

@inproceedings{zimmerman2018improving,
  title={Improving hate speech detection with deep learning ensembles},
  author={Zimmerman, Steven and Kruschwitz, Udo and Fox, Chris},
  booktitle={Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)},
  year={2018}
}

@inproceedings{gertner2019mitre,
    title = "{MITRE} at {S}em{E}val-2019 Task 5: Transfer Learning for Multilingual Hate Speech Detection",
    author = "Gertner, Abigail  and
      Henderson, John  and
      Merkhofer, Elizabeth  and
      Marsh, Amy  and
      Wellner, Ben  and
      Zarrella, Guido",
    booktitle = "Proceedings of the 13th International Workshop on Semantic Evaluation",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota, USA",
    publisher = "ACL",
    url = "https://www.aclweb.org/anthology/S19-2080",
    doi = "10.18653/v1/S19-2080",
    pages = "453--459",
}

@inproceedings{deriu2017leveraging,
 author = {Deriu, Jan and Lucchi, Aurelien and De Luca, Valeria and Severyn, Aliaksei and M\"{u}ller, Simon and Cieliebak, Mark and Hofmann, Thomas and Jaggi, Martin},
 title = {Leveraging Large Amounts of Weakly Supervised Data for Multi-Language Sentiment Classification},
 booktitle = {WWW},
 series = {WWW '17},
 year = {2017},
 isbn = {978-1-4503-4913-0},
 location = {Perth, Australia},
 pages = {1045--1052},
 numpages = {8},
 url = {https://doi.org/10.1145/3038912.3052611},
 doi = {10.1145/3038912.3052611},
 acmid = {3052611},
 publisher = {International World Wide Web Conferences Steering Committee},
 address = {Republic and Canton of Geneva, Switzerland},
}

@inproceedings{godin2015multimedia,
    title = "Multimedia Lab @ {ACL} {WNUT} {NER} Shared Task: Named Entity Recognition for Twitter Microposts using Distributed Word Representations",
    author = "Godin, Fr{\'e}deric  and
      Vandersmissen, Baptist  and
      De Neve, Wesley  and
      Van de Walle, Rik",
    booktitle = "Proceedings of the Workshop on Noisy User-generated Text",
    month = jul,
    year = "2015",
    address = "Beijing, China",
    publisher = "ACL",
    url = "https://www.aclweb.org/anthology/W15-4322",
    doi = "10.18653/v1/W15-4322",
    pages = "146--153",
}

@inproceedings{ptaszynski2017learning,
  title={Results of the PolEval 2019 Shared Task 6: First Dataset and Open Shared Task for Automatic Cyberbullying Detection in Polish Twitter},
  author={Ptaszynski, Michal and Pieciukiewicz, Agata and Dyba{\l}a, Pawe{\l}},
  booktitle={Proceedings of the PolEval2019 Workshop},
  pages={89},
  year={2019}
}

@article{king2001logistic,
  title={Logistic regression in rare events data},
  author={King, Gary and Zeng, Langche},
  journal={Political analysis},
  volume={9},
  number={2},
  pages={137--163},
  year={2001},
  publisher={Cambridge University Press}
}

@article{hochreiter1997long,
    author = {Hochreiter, Sepp and Schmidhuber, J{\"{u}}rgen},
    journal = {Neural computation},
    number = {8},
    pages = {1735--1780},
    publisher = {MIT Press},
    title = {{Long short-term memory}},
    volume = {9},
    year = {1997}
}

@inproceedings{jung2018assessing,
    author = {Jung, Soon-Gyo and An, Jisun and Kwak, Haewoon and Salminen, Joni and Jansen, Bernard Jim},
    booktitle = {Twelfth International AAAI Conference on Web and Social Media},
    title = {{Assessing the accuracy of four popular face recognition tools for inferring gender, age, and race}},
    year = {2018},
    url = {https://www.aaai.org/ocs/index.php/ICWSM/ICWSM18/paper/view/17839/17066}
}

@article{kingma2014dropout,
    abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different “thinned” networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets},
    address = {Valletta, Malta},
    note = {From Duplicate 10 (Software Framework for Topic Modelling with Large Corpora - $\backslash$v Reh$\backslash$r u$\backslash$v rek, Radim; Sojka, Petr)$\backslash$url{\{}http://is.muni.cz/publication/884893/en{\}}},
    author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
    institution = {Association for Computational Linguistics},
    isbn = {9781948087346},
    issn = {15337928},
    journal = {Journal of Machine Learning Research},
    keywords = {Deep learning,Model combination,Neural networks,Regularization},
    month = {may},
    number = {1},
    pages = {1929--1958},
    publisher = {JMLR. org},
    title = {{Dropout: A simple way to prevent neural networks from overfitting}},
    volume = {15},
    year = {2014}
}

@inproceedings{garg2019counterfactual,
 author = {Garg, Sahaj and Perot, Vincent and Limtiaco, Nicole and Taly, Ankur and Chi, Ed H. and Beutel, Alex},
 title = {Counterfactual Fairness in Text Classification Through Robustness},
 booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
 series = {AIES '19},
 year = {2019},
 isbn = {978-1-4503-6324-2},
 location = {Honolulu, HI, USA},
 pages = {219--226},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/3306618.3317950},
 doi = {10.1145/3306618.3317950},
 acmid = {3317950},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {counterfactual fairness, fairness, robustness, text classification},
}

@inproceedings{hardt2016equality,
    abstract = {We propose a criterion for discrimination against a specified sensitive attribute in supervised learning, where the goal is to predict some target based on available features. Assuming data about the predictor, target, and membership in the protected group are available, we show how to optimally adjust any learned predictor so as to remove discrimination according to our definition. Our framework also improves incentives by shifting the cost of poor classification from disadvantaged groups to the decision maker, who can respond by improving the classification accuracy. In line with other studies, our notion is oblivious: it depends only on the joint statistics of the predictor, the target and the protected attribute, but not on interpretation of individualfeatures. We study the inherent limits of defining and identifying biases based on such oblivious measures, outlining what can and cannot be inferred from different oblivious tests. We illustrate our notion using a case study of FICO credit scores.},
    author = {Hardt, Moritz and Price, Eric and Srebro, Nathan},
    booktitle = {Advances in Neural Information Processing Systems},
    issn = {10495258},
    pages = {3323--3331},
    title = {{Equality of opportunity in supervised learning}},
    url = {https://papers.nips.cc/paper/6374-equality-of-opportunity-in-supervised-learning.pdf},
    year = {2016}
}

@article{Zou05regularizationand,
    abstract = {We propose the elastic net, a new regularization and variable selection method. Real world data and a simulation study show that the elastic net often outperforms the lasso, while enjoying a similar sparsity of representation. In addition, the elastic net encourages a grouping effect, where strongly correlated predictors tend to be in or out of the model together. The elastic net is particularly useful when the number of predictors (p) is much bigger than the number of observations (n). By contrast, the lasso is not a very satisfactory variable selection method in the p≫n case. An algorithm called LARS-EN is proposed for computing elastic net regularization paths efficiently, much like algorithm LARS does for the lasso.},
    author = {Zou, Hui and Hastie, Trevor},
    doi = {10.1111/j.1467-9868.2005.00503.x},
    issn = {13697412},
    journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
    keywords = {Grouping effect,LARS algorithm,Lasso,P ≫ n problem,Penalization,Variable selection},
    number = {2},
    pages = {301--320},
    title = {{Regularization and variable selection via the elastic net}},
    volume = {67},
    year = {2005}
}

@inproceedings{cabrera2018gender,
    abstract = {Wikipedia is an important source of information in today's world. Yet, the lack of gender diversity in its community has been shown to affect the topics covered. Each Wikipedia article has a talk page that volunteer editors use to discuss proposed changes. Research on the gender bias has focused on article contribution and topic coverage, but not talk page activity. It has been suggested that the conflicts that take place in talk pages are especially intimidating for women, but this assertion has not been quantified yet. To fill this gap, we collected a dataset of all comments on Wikipedia talk pages, enriching it with gender information available from users who have chosen to disclose their gender on their user profiles or settings. Among the users active in talk pages, 49,387 indicated that they are male while only 5,996 indicated that they are female. The comments of these users make up for 4 million comments, approximately one quarter of all comments on Wikipedia. In addition, we observed that female participation varies by topic, reflecting traditional gender stereotypes: compared to Science, Technology, Engineering and Mathematics (STEM) topics, women were more active in categories such as Gender studies or Feminism. Results also indicate that a post on a talk page is 2.4$\backslash${\%} less likely to be replied to if the author is female. Likewise, reply probability varies from topic to topic. These results provide quantitative support for a gender bias in Wikipedia talk pages, and serve as a basis for discussing why overall female participation is low.},
    author = {Cabrera, Benjamin and Ross, Bj{\"{o}}rn and Dado, Marielle and Heisel, Maritta},
    booktitle = {12th International AAAI Conference on Web and Social Media, ICWSM 2018},
    isbn = {9781577357988},
    pages = {572--575},
    title = {{The gender gap in wikipedia talk pages}},
    url = {https://www.aaai.org/ocs/index.php/ICWSM/ICWSM18/paper/view/17829},
    year = {2018}
}

@inproceedings{waseem2016hateful,
    abstract = {Hate speech in the form of racist and sex-ist remarks are a common occurrence on social media. For that reason, many so-cial media services address the problem of identifying hate speech, but the defini-tion of hate speech varies markedly and is largely a manual effort (BBC, 2015; Lo-mas, 2015). We provide a list of criteria founded in critical race theory, and use them to an-notate a publicly available corpus of more than 16k tweets. We analyze the impact of various extra-linguistic features in con-junction with character n-grams for hate-speech detection. We also present a dic-tionary based the most indicative words in our data.},
    author = {Waseem, Zeerak and Hovy, Dirk},
    booktitle = {Proceedings of the NAACL student research workshop},
    doi = {10.18653/v1/n16-2013},
    pages = {88--93},
    title = {{Hateful Symbols or Hateful People? Predictive Features for Hate Speech Detection on Twitter}},
    url = {https://www.aclweb.org/anthology/N16-2013},
    year = {2016}
}

@inproceedings{yao2018dynamic,
    abstract = {Word evolution refers to the changing meanings and associations of words throughout time, as a byproduct of human language evolution. By studying word evolution, we can infer social trends and language constructs over different periods of human history. However, traditional techniques such as word representation learning do not adequately capture the evolving language structure and vocabulary. In this paper, we develop a dynamic statistical model to learn time-aware word vector representation. We propose a model that simultaneously learns time-aware embeddings and solves the resulting "alignment problem". This model is trained on a crawled NYTimes dataset. Additionally, we develop multiple intuitive evaluation strategies of temporal word embeddings. Our qualitative and quantitative tests indicate that our method not only reliably captures this evolution over time, but also consistently outperforms state-of-the-art temporal embedding approaches on both semantic accuracy and alignment quality.},
    author = {Yao, Zijun and Sun, Yifan and Ding, Weicong and Rao, Nikhil and Xiong, Hui},
    title = {Dynamic Word Embeddings for Evolving Semantic Discovery},
    booktitle = {Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining},
    series = {WSDM '18},
    year = {2018},
    isbn = {978-1-4503-5581-0},
    location = {Marina Del Rey, CA, USA},
    pages = {673--681},
    numpages = {9},
    url = {http://doi.acm.org/10.1145/3159652.3159703},
    doi = {10.1145/3159652.3159703},
    acmid = {3159703},
    publisher = {ACM},
    address = {New York, NY, USA},
    keywords = {dynamic word embeddings, word semantic analysis},
} 


@inproceedings{borkan2019nuanced,
    abstract = {Unintended bias in Machine Learning can manifest as systemic differences in performance for different demographic groups, potentially compounding existing challenges to fairness in society at large. In this paper, we introduce a suite of threshold-agnostic metrics that provide a nuanced view of this unintended bias, by considering the various ways that a classifier's score distribution can vary across designated groups. We also introduce a large new test set of online comments with crowd-sourced annotations for identity references. We use this to show how our metrics can be used to find new and potentially subtle unintended bias in existing public models.},
    author = {Borkan, Daniel and Dixon, Lucas and Sorensen, Jeffrey and Thain, Nithum and Vasserman, Lucy},
    booktitle = {The Web Conference 2019 - Companion of the World Wide Web Conference, WWW 2019},
    doi = {10.1145/3308560.3317593},
    isbn = {9781450366755},
    pages = {491--500},
    title = {{Nuanced metrics for measuring unintended bias with real data for text classification}},
    url = {https://dl.acm.org/citation.cfm?id=3317593},
    year = {2019}
}

@inproceedings{lai2015recurrent,
    abstract = {{\textcopyright} Copyright 2015, Association for the Advancement of Artificial Intelligence (www.aaa1.org). All rights reserved. Text classification is a foundational task in many NLP applications. Traditional text classifiers often rely on many human-designed features, such as dictionaries, knowledge bases and special tree kernels. In contrast to traditional methods, we introduce a recurrent convolutional neural network for text classification without human-designed features. In our model, we apply a recurrent structure to capture contextual information as far as possible when learning word representations, which may introduce considerably less noise compared to traditional window-based neural networks. We also employ a max-pooling layer that automatically judges which words play key roles in text classification to capture the key components in texts. We conduct experiments on four commonly used datasets. The experimental results show that the proposed method outperforms the state-of-the-art methods on several datasets, particularly on document-level datasets.},
    author = {Lai, Siwei and Xu, Liheng and Liu, Kang and Zhao, Jun},
    booktitle = {Proceedings of the National Conference on Artificial Intelligence},
    isbn = {9781577357018},
    pages = {2267--2273},
    title = {{Recurrent convolutional neural networks for text classification}},
    volume = {3},
    year = {2015}
}

@inproceedings{shen2018wasserstein,
    abstract = {Domain adaptation aims at generalizing a high-performance learner on a target domain via utilizing the knowledge distilled from a source domain which has a different but related data distribution. One solution to domain adaptation is to learn domain invariant feature representations while the learned representations should also be discriminative in prediction. To learn such representations, domain adaptation frameworks usually include a domain invariant representation learning approach to measure and reduce the domain discrepancy, as well as a discriminator for classification. Inspired by Wasserstein GAN, in this paper we propose a novel approach to learn domain invariant feature representations, namely Wasserstein Distance Guided Representation Learning (WDGRL). WDGRL utilizes a neural network, denoted by the domain critic, to estimate empirical Wasserstein distance between the source and target samples and optimizes the feature extractor network to minimize the estimated Wasserstein distance in an adversarial manner. The theoretical advantages of Wasserstein distance for domain adaptation lie in its gradient property and promising generalization bound. Empirical studies on common sentiment and image classification adaptation datasets demonstrate that our proposed WDGRL outperforms the state-of-the-art domain invariant representation learning approaches.},
    author = {Shen, Jian and Qu, Yanru and Zhang, Weinan and Yu, Yong},
    booktitle = {32nd AAAI Conference on Artificial Intelligence, AAAI 2018},
    isbn = {9781577358008},
    pages = {4058--4065},
    title = {{Wasserstein distance guided representation learning for domain adaptation}},
    year = {2018}
}

@inproceedings{hovy2018capturing,
    abstract = {Dialects are one of the main drivers of language variation, a major challenge for natural language processing tools. In most languages, dialects exist along a continuum, and are commonly discretized by combining the extent of several preselected linguistic variables. However , the selection of these variables is theory-driven and itself insensitive to change. We use Doc2Vec on a corpus of 16.8M anonymous online posts in the German-speaking area to learn continuous document representations of cities. These representations capture continuous regional linguistic distinctions, and can serve as input to downstream NLP tasks sensitive to regional variation. By incorporating geographic information via retrofitting and ag-glomerative clustering with structure, we recover dialect areas at various levels of gran-ularity. Evaluating these clusters against an existing dialect map, we achieve a match of up to 0.77 V-score (harmonic mean of cluster completeness and homogeneity). Our results show that representation learning with retrofitting offers a robust general method to automatically expose dialectal differences and regional variation at a finer granularity than was previously possible.},
    author = {Hovy, Dirk and Purschke, Christoph},
    booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
    doi = {10.18653/v1/d18-1469},
    pages = {4383--4394},
    title = {{Capturing Regional Variation with Distributed Place Representations and Geographic Retrofitting}},
    year = {2019}
}

@article{hahnloser2000digital,
    author = {Hahnloser, Richard H R and Sarpeshkar, Rahul and Mahowald, Misha A and Douglas, Rodney J and Seung, H Sebastian},
    journal = {Nature},
    number = {6789},
    pages = {947},
    publisher = {Nature Publishing Group},
    title = {{Digital selection and analogue amplification coexist in a cortex-inspired silicon circuit}},
    volume = {405},
    year = {2000}
}

@inproceedings{kim2017domain,
    abstract = {An important problem in domain adapta-tion is to quickly generalize to a new do-main with limited supervision given K ex-isting domains. One approach is to re-train a global model across all K + 1 do-mains using standard techniques, for in-stance Daum{\'{e}} III (2009). However, it is desirable to adapt without having to re-estimate a global model from scratch each time a new domain with potentially new intents and slots is added. We describe a solution based on attending an ensemble of domain experts. We assume K domain-specific intent and slot models trained on respective domains. When given domain K + 1, our model uses a weighted combi-nation of the K domain experts' feedback along with its own opinion to make predic-tions on the new domain. In experiments, the model significantly outperforms base-lines that do not use domain adaptation and also performs better than the full re-training approach.},
    author = {Kim, Young Bum and Stratos, Karl and Kim, Dongchan},
    booktitle = {ACL 2017 - 55th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference (Long Papers)},
    doi = {10.18653/v1/P17-1060},
    isbn = {9781945626753},
    pages = {643--653},
    title = {{Domain attention with an ensemble of experts}},
    volume = {1},
    year = {2017}
}

@inproceedings{berg2012empirical,
    title = "An Empirical Investigation of Statistical Significance in {NLP}",
    author = "Berg-Kirkpatrick, Taylor  and
      Burkett, David  and
      Klein, Dan",
    booktitle = "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning",
    month = jul,
    year = "2012",
    address = "Jeju Island, Korea",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D12-1091",
    pages = "995--1005",
}

@inproceedings{dror2018hitchhiker,
    abstract = {Statistical significance testing is a standard statistical tool designed to ensure that experimental results are not coincidental. In this opin-ion/theoretical paper we discuss the role of statistical significance testing in Natural Language Processing (NLP) research. We establish the fundamental concepts of significance testing and discuss the specific aspects of NLP tasks, experimental setups and evaluation measures that affect the choice of significance tests in NLP research. Based on this discussion, we propose a simple practical protocol for statistical significance test selection in NLP setups and accompany this protocol with a brief survey of the most relevant tests. We then survey recent empirical papers published in ACL and TACL during 2017 and show that while our community assigns great value to experimental results , statistical significance testing is often ignored or misused. We conclude with a brief discussion of open issues that should be properly addressed so that this important tool can be applied in NLP research in a statistically sound manner 1 .},
    author = {Dror, Rotem and Baumer, Gili and Shlomov, Segev and Reichart, Roi},
    booktitle = {ACL 2018 - 56th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference (Long Papers)},
    isbn = {9781948087322},
    pages = {1383--1392},
    title = {{The hitchhiker's guide to testing statistical significance in natural language processing}},
    volume = {1},
    year = {2018}
}

@article{vallender1974calculation,
    author = {Vallender, S S},
    doi = {10.1137/1118101},
    issn = {0040-585X},
    journal = {Theory of Probability {\&} Its Applications},
    month = {sep},
    number = {4},
    pages = {784--786},
    publisher = {SIAM},
    title = {{Calculation of the Wasserstein Distance Between Probability Distributions on the Line}},
    url = {http://epubs.siam.org/doi/10.1137/1118101},
    volume = {18},
    year = {1974}
}

@inproceedings{dufour2016tracking,
    abstract = {{\textcopyright} 2016 IEEE. Automatically translating textual documents from one language to another inevitably results in translation errors. In addition to language specificities, this automatic translation appears more difficult in the context of spoken dialogues since, for example, the language register is far from 'clean speech'. Speech analytics suffer from these translation errors. To tackle this difficulty, a solution consists in mapping translations into a space of hidden topics. In the classical topic-based representation obtained from a Latent Dirichlet Allocation (LDA), distribution of words into each topic is estimated automatically. Nonetheless, the targeted classes are ignored in the particular context of a classification task. In the DSTC5 main task, this targeted class information is crucial, the main objective being to track dialog states for sub-dialog segments. For this challenge, we propose to apply an original topic-based representation for each sub-dialogue based not only on the sub-dialogue content itself (words), but also on the dialogue state related to the sub-dialogue. This original representation is based on the Author-Topic (AT) model, previously successfully applied on a different classification task. Promising results confirmed the interest of such a method, the AT model reaching performance slightly better in terms of F-measure than baseline ones given by the task's organizers.},
    author = {Dufour, Richard and Morchid, Mohamed and Parcollet, Titouan},
    booktitle = {2016 IEEE Workshop on Spoken Language Technology, SLT 2016 - Proceedings},
    doi = {10.1109/SLT.2016.7846316},
    isbn = {9781509049035},
    keywords = {Author-Topic Model,Dialog State Tracking,Sub-dialog Level},
    organization = {IEEE},
    pages = {544--551},
    title = {{Tracking dialog states using an Author-Topic based representation}},
    year = {2017}
}

@inproceedings{Finkel09,
    title = "Hierarchical {B}ayesian Domain Adaptation",
    author = "Finkel, Jenny Rose  and
      Manning, Christopher D.",
    booktitle = "Proceedings of Human Language Technologies: The 2009 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics",
    month = jun,
    year = "2009",
    address = "Boulder, Colorado",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N09-1068",
    pages = "602--610",
}

@article{schuster1997bidirectional,
    abstract = {In the first part of this paper, a regular recurrent neural$\backslash$nnetwork (RNN) is extended to a bidirectional recurrent neural network$\backslash$n(BRNN). The BRNN can be trained without the limitation of using input$\backslash$ninformation just up to a preset future frame. This is accomplished by$\backslash$ntraining it simultaneously in positive and negative time direction.$\backslash$nStructure and training procedure of the proposed network are explained.$\backslash$nIn regression and classification experiments on artificial data, the$\backslash$nproposed structure gives better results than other approaches. For real$\backslash$ndata, classification experiments for phonemes from the TIMIT database$\backslash$nshow the same tendency. In the second part of this paper, it is shown$\backslash$nhow the proposed bidirectional structure can be easily modified to allow$\backslash$nefficient estimation of the conditional posterior probability of$\backslash$ncomplete symbol sequences without making any explicit assumption about$\backslash$nthe shape of the distribution. For this part, experiments on real data$\backslash$nare reported},
    author = {Schuster, Mike and Paliwal, Kuldip K.},
    doi = {10.1109/78.650093},
    issn = {1053587X},
    journal = {IEEE Transactions on Signal Processing},
    keywords = {Recurrent neural networks},
    number = {11},
    pages = {2673--2681},
    publisher = {IEEE},
    title = {{Bidirectional recurrent neural networks}},
    volume = {45},
    year = {1997}
}

@inproceedings{mihalcea2012word,
    abstract = {In this paper we introduce the novel task of "word epoch disambiguation," defined as the problem of identifying changes in word usage over time. Through experiments run using word usage examples collected from three major periods of time (1800, 1900, 2000), we show that the task is feasible, and significant differences can be observed between occurrences of words in different periods of time. {\textcopyright} 2012 Association for Computational Linguistics.},
    title = "Word Epoch Disambiguation: Finding How Words Change Over Time",
    author = "Mihalcea, Rada  and
      Nastase, Vivi",
    booktitle = "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jul,
    year = "2012",
    address = "Jeju Island, Korea",
    publisher = "Association for Computational Linguistics",
    pages = "259--263",
}

@inproceedings{popescu2015semeval,
    title = "{S}em{E}val 2015, Task 7: Diachronic Text Evaluation",
    author = "Popescu, Octavian  and
      Strapparava, Carlo",
    booktitle = "Proceedings of the 9th International Workshop on Semantic Evaluation ({S}em{E}val 2015)",
    month = jun,
    year = "2015",
    address = "Denver, Colorado",
    publisher = "Association for Computational Linguistics",
    doi = "10.18653/v1/S15-2147",
    pages = "870--878",
}

@inproceedings{kutuzov2017tracing,
    title = "Tracing armed conflicts with diachronic word embedding models",
    author = "Kutuzov, Andrey  and
      Velldal, Erik  and
      {\O}vrelid, Lilja",
    booktitle = "Proceedings of the Events and Stories in the News Workshop",
    month = aug,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    doi = "10.18653/v1/W17-2705",
    pages = "31--36",
    abstract = "Recent studies have shown that word embedding models can be used to trace time-related (diachronic) semantic shifts in particular words. In this paper, we evaluate some of these approaches on the new task of predicting the dynamics of global armed conflicts on a year-to-year basis, using a dataset from the conflict research field as the gold standard and the Gigaword news corpus as the training data. The results show that much work still remains in extracting {`}cultural{'} semantic shifts from diachronic word embedding models. At the same time, we present a new task complete with an evaluation set and introduce the {`}anchor words{'} method which outperforms previous approaches on this set.",
}

@inproceedings{johannsen2015cross,
    abstract = {Most computational sociolinguistics studies have focused on phonological and lexical variation. We present the first large-scale study of syntactic variation among demographic groups (age and gender) across several languages. We harvest data from online user-review sites and parse it with universal dependencies. We show that several age and gender-specific variations hold across languages, for example that women are more likely to use VP conjunctions.},
    author = {Johannsen, Anders and Hovy, Dirk and S{\o}gaard, Anders},
    booktitle = {Proceedings of the Nineteenth Conference on Computational Natural Language Learning},
    doi = {10.18653/v1/k15-1011},
    pages = {103--112},
    title = {{Cross-lingual syntactic variation over age and gender}},
    year = {2015}
}

@unpublished{miller2008manual,
    note = {Unpublished manuscript. Retrieved from: $\backslash$url{\{}https://casaa.unm.edu/download/misc.pdf{\}}},
    author = {Miller, William R and Moyers, Theresa B and Ernst, Denise and Amrhein, Paul},
    booktitle = {Substance Abuse, and Addictions},
    title = {{Motivational Interviewing Skill Code Manual v 2.1}},
    url = {https://casaa.unm.edu/download/misc.pdf},
    year = {2008}
}

@inproceedings{zhang2018mitigating,
    abstract = {Machine learning is a tool for building models that accurately represent input training data. When undesired biases concerning demographic groups are in the training data, well-trained models will reflect those biases. We present a framework for mitigating such biases by including a variable for the group of interest and simultaneously learning a predictor and an adversary. The input to the network X, here text or census data, produces a prediction Y, such as an analogy completion or income bracket, while the adversary tries to model a protected variable Z, here gender or zip code. The objective is to maximize the predictor's ability to predict Y while minimizing the adversary's ability to predict Z. Applied to analogy completion, this method results in accurate predictions that exhibit less evidence of stereotyping Z. When applied to a classification task using the UCI Adult (Census) Dataset, it results in a predictive model that does not lose much accuracy while achieving very close to equality of odds (Hardt, et al., 2016). The method is flexible and applicable to multiple definitions of fairness as well as a wide range of gradient-based learning models, including both regression and classification tasks.},
    author = {Zhang, Brian Hu and Lemoine, Blake and Mitchell, Margaret},
    booktitle = {AIES 2018 - Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
    doi = {10.1145/3278721.3278779},
    isbn = {9781450360128},
    keywords = {adversarial learning,debiasing,multi-task learning,unbiasing},
    pages = {335--340},
    title = {{Mitigating Unwanted Biases with Adversarial Learning}},
    url = {http://www.aies-conference.com/wp-content/papers/main/AIES-19{\_}paper{\_}35.pdf},
    year = {2018}
}

@inproceedings{volkova2015inferring,
    address = {Austin, TX},
    author = {Volkova, Svitlana and Bachrach, Yoram and Armstrong, Michael and Sharma, Vijay},
    booktitle = {AAAI Conference on Artificial Intelligence (AAAI)},
    month = {jan},
    title = {{Inferring Latent User Properties from Texts Published in Social Media}},
    year = {2015}
}

@inproceedings{gao2015more,
    abstract = {Is one demographic group inherently different from another? Does a group express the same sentiment both in private and public? How can we compare the sentiments of different groups composed of multiple attributes? In this paper, we take an interdisciplinary approach towards mining the patterns of textual sentiments and metadata. First, we look into several existing hypotheses in social science on the interplay between user characteristics and sentiments, as well as the related evidence in the field of social network data analysis. Second, we present a dataset with unique features (Facebook users' chats and posts in multiple languages) and a procedure to process the data. Third, we test our hypotheses on this dataset and interpret the results. Fourth, under the subgroup-discovery paradigm, we present an approach with two algorithms that generalizes single-attribute testing. This approach provides more detailed insight into the relationships among attributes, and reveals interesting attribute- value combinations with distinct sentiments. Furthermore, it offers novel hypotheses for examination in future studies.},
    author = {Gao, Bo and Berendt, Bettina and Vanschoren, Joaquin},
    booktitle = {Proceedings of the 2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining, ASONAM 2015},
    doi = {10.1145/2808797.2809421},
    isbn = {9781450338547},
    month = {aug},
    pages = {605--610},
    title = {{Who is more positive in private? Analyzing sentiment differences across privacy levels and demographic factors in facebook chats and posts}},
    year = {2015}
}

@article{chen2016adversarial,
    abstract = { In recent years great success has been achieved in sentiment classification for English, thanks in part to the availability of copious annotated resources. Unfortunately, most languages do not enjoy such an abundance of labeled data. To tackle the sentiment classification problem in low-resource languages without adequate annotated data, we propose an Adversarial Deep Averaging Network (ADAN 1 ) to transfer the knowledge learned from labeled data on a resource-rich source language to low-resource languages where only unlabeled data exist. ADAN has two discriminative branches: a sentiment classifier and an adversarial language discriminator. Both branches take input from a shared feature extractor to learn hidden representations that are simultaneously indicative for the classification task and invariant across languages. Experiments on Chinese and Arabic sentiment classification demonstrate that ADAN significantly outperforms state-of-the-art systems. },
    author = {Chen, Xilun and Sun, Yu and Athiwaratkun, Ben and Cardie, Claire and Weinberger, Kilian},
    doi = {10.1162/tacl_a_00039},
    journal = {Transactions of the Association for Computational Linguistics},
    pages = {557--570},
    title = {{Adversarial Deep Averaging Networks for Cross-Lingual Sentiment Classification}},
    volume = {6},
    year = {2018}
}

@inproceedings{sun2019mitigating,
    abstract = {As Natural Language Processing (NLP) and Machine Learning (ML) tools rise in popularity, it becomes increasingly vital to recognize the role they play in shaping societal biases and stereotypes. Although NLP models have shown success in modeling various applications, they propagate and may even amplify gender bias found in text corpora. While the study of bias in artificial intelligence is not new, methods to mitigate gender bias in NLP are relatively nascent. In this paper, we review contemporary studies on recognizing and mitigating gender bias in NLP. We discuss gender bias based on four forms of representation bias and analyze methods recognizing gender bias. Furthermore, we discuss the advantages and drawbacks of existing gender debiasing methods. Finally, we discuss future studies for recognizing and mitigating gender bias in NLP.},
    archivePrefix = {arXiv},
    arxivId = {1906.08976},
    author = {Sun, Tony and Gaut, Andrew and Tang, Shirlyn and Huang, Yuxin and ElSherief, Mai and Zhao, Jieyu and Mirza, Diba and Belding, Elizabeth and Chang, Kai-Wei and Wang, William Yang},
    booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
    eprint = {1906.08976},
    pages = {1630--1640},
    title = {{Mitigating Gender Bias in Natural Language Processing: Literature Review}},
    url = {http://arxiv.org/abs/1906.08976},
    year = {2019}
}

@inproceedings{huang2018examining,
    title = "Examining Temporality in Document Classification",
    author = "Huang, Xiaolei  and
      Paul, Michael J.",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P18-2110",
    doi = "10.18653/v1/P18-2110",
    pages = "694--699",
    abstract = "Many corpora span broad periods of time. Language processing models trained during one time period may not work well in future time periods, and the best model may depend on specific times of year (e.g., people might describe hotels differently in reviews during the winter versus the summer). This study investigates how document classifiers trained on documents from certain time intervals perform on documents from other time intervals, considering both seasonal intervals (intervals that repeat across years, e.g., winter) and non-seasonal intervals (e.g., specific years). We show experimentally that classification performance varies over time, and that performance can be improved by using a standard domain adaptation approach to adjust for changes in time.",
}

@inproceedings{rudolph2018dynamic,
    abstract = {Word embeddings are a powerful approach for unsupervised analysis of language. Recently, Rudolph et al. (2016) developed exponential family embeddings, which cast word embeddings in a probabilistic framework. Here, we develop dynamic embeddings, building on exponential family embeddings to capture how the meanings of words change over time. We use dynamic embeddings to analyze three large collections of historical texts: the U.S. Senate speeches from 1858 to 2009, the history of computer science ACM abstracts from 1951 to 2014, and machine learning papers on the Arxiv from 2007 to 2015. We find dynamic embeddings provide better fits than classical embeddings and capture interesting patterns about how language changes.},
    author = {Rudolph, Maja and Blei, David},
    booktitle = {Proceedings of the 2018 World Wide Web Conference on World Wide Web},
    doi = {10.1145/3178876.3185999},
    organization = {International World Wide Web Conferences Steering Committee},
    pages = {1003--1011},
    title = {{Dynamic Embeddings for Language Evolution}},
    year = {2018}
}

@inproceedings{hamilton2016diachronic,
    abstract = {Understanding how words change their meanings over time is key to models of language and cultural evolution, but historical data on meaning is scarce, making theories hard to develop and test. Word embeddings show promise as a diachronic tool, but have not been carefully evaluated. We develop a robust methodology for quantifying semantic change by evaluating word embeddings (PPMI, SVD, word2vec) against known historical changes. We then use this methodology to reveal statistical laws of semantic evolution. Using six historical corpora spanning four languages and two centuries, we propose two quantitative laws of semantic change: (i) the law of conformity---the rate of semantic change scales with an inverse power-law of word frequency; (ii) the law of innovation---independent of frequency, words that are more polysemous have higher rates of semantic change.},
    author = {Hamilton, William L. and Leskovec, Jure and Jurafsky, Dan},
    booktitle = {54th Annual Meeting of the Association for Computational Linguistics, ACL 2016 - Long Papers},
    isbn = {9781510827585},
    pages = {1489--1501},
    title = {{Diachronie word embeddings reveal statistical laws of semantic change}},
    volume = {3},
    year = {2016}
}

@article{borsari2012addressing,
    abstract = {Objective: Over the past 2 decades, colleges and universities have seen a large increase in the number of students referred to the administration for alcohol policies violations. However, a substantial portion of mandated students may not require extensive treatment. Stepped care may maximize treatment efficiency and greatly reduce the demands on campus alcohol programs. Method: Participants in the study (N = 598) were college students mandated to attend an alcohol program following a campus-based alcohol citation. All participants received Step 1: a 15-min brief advice session that included the provision of a booklet containing advice to reduce drinking. Participants were assessed 6 weeks after receiving the brief advice, and those who continued to exhibit risky alcohol use (n = 405) were randomized to Step 2, a 60-to 90-min brief motivational intervention (n = 211), or an assessment-only control (n = 194). Follow-up assessments were conducted 3, 6, and 9 months after Step 2. Results: Results indicated that the participants who received a brief motivational intervention showed a significantly reduced number of alcohol-related problems compared to those who received assessment only, despite no significant group differences in alcohol use. In addition, low-risk drinkers (n = 102; who reported low alcohol use and related harms at 6-week follow-up and were not randomized to stepped care) showed a stable alcohol use pattern throughout the follow-up period, indicating they required no additional intervention. Conclusion: Stepped care is an efficient and cost-effective method to reduce harms associated with alcohol use by mandated students. ? 2012 American Psychological Association.},
    author = {Borsari, Brian and Hustad, John T.P. and Mastroleo, Nadine R. and Tevyaw, Tracy O.Leary and Barnett, Nancy P. and Kahler, Christopher W. and Short, Erica Eaton and Monti, Peter M.},
    doi = {10.1037/a0029902},
    issn = {19392117},
    journal = {Journal of Consulting and Clinical Psychology},
    keywords = {alcohol,college,intervention,mandated students,stepped care},
    number = {6},
    pages = {1062--1074},
    publisher = {American Psychological Association},
    title = {{Addressing alcohol use and problems in mandated college students: A randomized clinical trial using stepped care}},
    volume = {80},
    year = {2012}
}

@inproceedings{jiang2007instance,
    abstract = {Domain adaptation is an important problem in natural language processing (NLP) due to the lack of labeled data in novel domains. In this paper, we study the domain adaptation problem from the instance weighting perspective. We formally analyze and characterize the domain adaptation problem from a distributional view, and show that there are two distinct needs for adaptation, corresponding to the different distributions of instances and classification functions in the source and the target domains. We then propose a general instance weighting framework for domain adaptation. Our empirical results on three NLP tasks show that incorporating and exploiting more information from the target domain through instance weighting is effective. {\textcopyright} 2007 Association for Computational Linguistics.},
    author = {Jiang, Jing and Zhai, Cheng Xiang},
    booktitle = {ACL 2007 - Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics},
    isbn = {9781932432862},
    pages = {264--271},
    title = {{Instance weighting for domain adaptation in NLP}},
    year = {2007}
}

@inproceedings{mei2017neural,
    abstract = {Many events occur in the world. Some event types are stochastically excited or inhibited---in the sense of having their probabilities elevated or decreased---by patterns in the sequence of previous events. Discovering such patterns can help us predict which type of event will happen next and when. We model streams of discrete events in continuous time, by constructing a neurally self-modulating multivariate point process in which the intensities of multiple event types evolve according to a novel continuous-time LSTM. This generative model allows past events to influence the future in complex and realistic ways, by conditioning future event intensities on the hidden state of a recurrent neural network that has consumed the stream of past events. Our model has desirable qualitative properties. It achieves competitive likelihood and predictive accuracy on real and synthetic datasets, including under missing-data conditions.},
    author = {Mei, Hongyuan and Eisner, Jason},
    booktitle = {Advances in Neural Information Processing Systems},
    issn = {10495258},
    pages = {6755--6765},
    title = {{The neural Hawkes process: A neurally self-modulating multivariate point process}},
    volume = {2017-Decem},
    year = {2017}
}

@article{crookes1990utterance,
    abstract = {Discusses prominent base units of discourse analysis designed for dealing with structural characteristics of second-language discourse, and offers arguments in favor of selecting the utterance as a valuable base unit. (95 references) (GLR)},
    author = {Crookes, Graham},
    doi = {10.1093/applin/11.2.183},
    issn = {01426001},
    journal = {Applied Linguistics},
    number = {2},
    pages = {183--199},
    publisher = {Oxford University Press},
    title = {{The utterance, and other basic units for second language discourse analysis}},
    volume = {11},
    year = {1990}
}

@inproceedings{kulkarni2015statistically,
    abstract = {We propose a new computational approach for tracking and detecting statistically significant linguistic shifts in the meaning and usage of words. Such linguistic shifts are especially prevalent on the Internet, where the rapid exchange of ideas can quickly change a word's meaning. Our meta-analysis approach constructs property time series of word usage, and then uses statistically sound change point detection algorithms to identify significant linguistic shifts. We consider and analyze three approaches of increasing complexity to generate such linguistic property time series, the culmination of which uses distributional characteristics inferred from word co-occurrences. Using recently proposed deep neural language models, we first train vector representations of words for each time period. Second, we warp the vector spaces into one unified coordinate system. Finally, we construct a distance-based distributional time series for each word to track it's linguistic displacement over time. We demonstrate that our approach is scalable by tracking linguistic change across years of micro-blogging using Twitter, a decade of product reviews using a corpus of movie reviews from Amazon, and a century of written books using the Google Book-ngrams. Our analysis reveals interesting patterns of language usage change commensurate with each medium.},
    author = {Kulkarni, Vivek and Al-Rfou, Rami and Perozzi, Bryan and Skiena, Steven},
    booktitle = {WWW 2015 - Proceedings of the 24th International Conference on World Wide Web},
    doi = {10.1145/2736277.2741627},
    isbn = {9781450334693},
    keywords = {Computational Linguistics,Web Mining},
    organization = {International World Wide Web Conferences Steering Committee},
    pages = {625--635},
    title = {{Statistically significant detection of linguistic change}},
    year = {2015}
}

@inproceedings{pennington2014glove,
    abstract = {Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arith- metic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global log- bilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word co- occurrence matrix, rather than on the en- tire sparse matrix or on individual context windows in a large corpus. On a recent word analogy task our model obtains 75{\%} accuracy, an improvement of 11{\%} over Mikolov et al. (2013). It also outperforms related word vector models on similarity tasks and named entity recognition.},
    author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher D.},
    booktitle = {EMNLP 2014 - 2014 Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference},
    isbn = {9781937284961},
    pages = {1532--1543},
    title = {{GloVe: Global vectors for word representation}},
    url = {http://www.aclweb.org/anthology/D14-1162},
    year = {2014}
}

@article{chung2014empirical,
    author = {Chung, Junyoung and Gulcehre, Caglar and Cho, KyungHyun and Bengio, Yoshua},
    journal = {arXiv preprint arXiv:1412.3555},
    title = {{Empirical evaluation of gated recurrent neural networks on sequence modeling}},
    url = {https://arxiv.org/pdf/1412.3555.pdf},
    year = {2014}
}

@article{Caruana1997multitask,
    author="Caruana, Rich",
    title="Multitask Learning",
    journal="Machine Learning",
    year="1997",
    month="Jul",
    day="01",
    volume="28",
    number="1",
    pages="41--75",
    abstract="Multitask Learning is an approach to inductive transfer that improves generalization by using the domain information contained in the training signals of related tasks as an inductive bias. It does this by learning tasks in parallel while using a shared representation; what is learned for each task can help other tasks be learned better. This paper reviews prior work on MTL, presents new evidence that MTL in backprop nets discovers task relatedness without the need of supervisory signals, and presents new results for MTL with k-nearest neighbor and kernel regression. In this paper we demonstrate multitask learning in three domains. We explain how multitask learning works, and show that there are many opportunities for multitask learning in real domains. We present an algorithm and results for multitask learning with case-based methods like k-nearest neighbor and kernel regression, and sketch an algorithm for multitask learning in decision trees. Because multitask learning works, can be applied to many different kinds of domains, and can be used with different learning algorithms, we conjecture there will be many opportunities for its use on real-world problems.",
    issn="1573-0565",
    doi="10.1023/A:1007379606734",
    url="https://doi.org/10.1023/A:1007379606734"
}

@article{magill2014technical,
    abstract = {{\textcopyright} 2014 American Psychological Association. Objective: The technical hypothesis of motivational interviewing (MI) posits that therapist-implemented MI skills are related to client speech regarding behavior change and that client speech predicts client outcome. The current meta-analysis is the first aggregate test of this proposed causal model. Method: A systematic literature review, using stringent inclusion criteria, identified 16 reports describing 12 primary studies. We used review methods to calculate the inverse-variance-weighted pooled correlation coefficient for the therapist-to-client and the client-to-outcome paths across multiple targeted behaviors (i.e., alcohol or illicit drug use, other addictive behaviors). Results: Therapist MI-consistent skills were correlated with more client language in favor of behavior change (i.e., change talk; r = .26, p {\textless} .0001), but not less client language against behavior change (i.e., sustain talk; r = .10, p = .09). MI-inconsistent skills were associated with less change talk (r = -.17, p = .001) as well as more sustain talk (r = .07, p = .009). Among these studies, client change talk was not associated with follow-up outcome (r = .06, p = .41), but sustain talk was associated with worse outcome (r = -.24, p = .001). In addition, studies examining composite client language (e.g., an average of negative and positive statements) showed an overall positive relationship with client behavior change (r = .12, p = .006; k = 6). Conclusions: This meta-analysis provides an initial test and partial support for a key causal model of MI efficacy. Recommendations for MI practitioners, clinical supervisors, and process researchers are provided.},
    author = {Magill, Molly and Gaume, Jacques and Apodaca, Timothy R. and Walthers, Justin and Mastroleo, Nadine R. and Borsari, Brian and Longabaugh, Richard},
    journal = {Journal of Consulting and Clinical Psychology},
    doi = {10.1037/a0036833},
    issn = {19392117},
    keywords = {Change talk,Meta-analysis,Motivational interviewing,Sustain talk,Therapy process},
    number = {6},
    pages = {973--983},
    publisher = {American Psychological Association},
    title = {{The technical hypothesis of motivational interviewing: A meta-analysis of MI's key causal model}},
    volume = {82},
    year = {2014}
}

@book{Ullmann62,
    abstract = {Ullman, S. (1962). Semantics: An Introduction to the Science of Meaning. Oxford: Basil Blackwell.},
    address = {Oxford},
    author = {Potter, Simeon and Ullmann, Stephen},
    booktitle = {The Modern Language Review},
    doi = {10.2307/3720401},
    issn = {00267937},
    pages = {79},
    publisher = {Basil Blackwell},
    title = {{Semantics. An Introduction to the Science of Meaning}},
    volume = {58},
    year = {1963}
}

@inproceedings{kim2014temporal,
    abstract = {We provide a method for automatically detecting change in language across time through a chronologically trained neural language model. We train the model on the Google Books Ngram corpus to obtain word vector representations specific to each year, and identify words that have changed significantly from 1900 to 2009. The model identifies words such as "cell" and "gay" as having changed during that time period. The model simultaneously identifies the specific years during which such words underwent change.},
    author = {Kim, Yoon and Chiu, Yi-I and Hanaki, Kentaro and Hegde, Darshan and Petrov, Slav},
    booktitle = {Proceedings of the ACL 2014 Workshop on Language Technologies and Computational Social Science},
    doi = {10.3115/v1/w14-2517},
    pages = {61--65},
    title = {{Temporal Analysis of Language through Neural Language Models}},
    year = {2015}
}

@article{bender2018data,
    abstract = {In this paper, we propose data statements as a design solution and professional practice for natural language processing technologists, in both research and development. Through the adoption and widespread use of data statements, the field can begin to address critical scientific and ethical issues that result from the use of data from certain populations in the development of technology for other populations. We present a form that data statements can take and explore the implications of adopting them as part of regular practice. We argue that data statements will help alleviate issues related to exclusion and bias in language technology, lead to better precision in claims about how natural language processing research can generalize and thus better engineering results, protect companies from public embarrassment, and ultimately lead to language technology that meets its users in their own preferred linguistic style and furthermore does not misrepresent them to others.},
    author = {Bender, Emily M. and Friedman, Batya},
    doi = {10.1162/tacl_a_00041},
    journal = {Transactions of the Association for Computational Linguistics},
    pages = {587--604},
    title = {{Data Statements for Natural Language Processing: Toward Mitigating System Bias and Enabling Better Science}},
    url = {https://www.aclweb.org/anthology/Q18-1041},
    volume = {6},
    year = {2018}
}

@article{Bickel09,
    abstract = {We address classification problems for which the training instances$\backslash$nare governed by an input distribution that is allowed to differ$\backslash$narbitrarily from the test distribution---problems also referred to as$\backslash$nclassification under covariate shift.  We derive a solution that is$\backslash$npurely discriminative: neither training nor test distribution are$\backslash$nmodeled explicitly.  The problem of learning under covariate shift can$\backslash$nbe written as an integrated optimization problem. Instantiating the$\backslash$ngeneral optimization problem leads to a kernel logistic regression and$\backslash$nan exponential model classifier for covariate shift.  The optimization$\backslash$nproblem is convex under certain conditions; our findings also clarify$\backslash$nthe relationship to the known kernel mean matching procedure.  We$\backslash$nreport on experiments on problems of spam filtering, text$\backslash$nclassification, and landmine detection.},
    author = {Bickel, Steffen and Br{\"{u}}ckner, Michael and Scheffer, Tobias},
    issn = {15324435},
    journal = {Journal of Machine Learning Research},
    keywords = {Covariate shift,Discriminative learning,Transfer learning},
    month = {sep},
    pages = {2137--2155},
    title = {{Discriminative learning under covariate shift}},
    volume = {10},
    year = {2009}
}

@article{10.7717/peerj-cs.156,
    abstract = {The increased interest in analyzing and explaining gender inequalities in tech, media, and academia highlights the need for accurate inference methods to predict a person's gender from their name. Several such services exist that provide access to large databases of names, often enriched with information from social media profiles, culture-specific rules, and insights from sociolinguistics. We compare and benchmark five name-to-gender inference services by applying them to the classification of a test data set consisting of 7,076 manually labeled names. The compiled names are analyzed and characterized according to their geographical and cultural origin. We define a series of performance metrics to quantify various types of classification errors, and define a parameter tuning procedure to search for optimal values of the services' free parameters. Finally, we perform benchmarks of all services under study regarding several scenarios where a particular metric is to be optimized.},
    author = {Santamar{\'{i}}a, Luc{\'{i}}a and Mihaljevi{\'{c}}, Helena},
    doi = {10.7717/peerj-cs.156},
    issn = {23765992},
    journal = {PeerJ Computer Science},
    keywords = {Bibliometrics,Classification algorithms,Gender analysis,Name-based gender inference,Performance evaluation,Scientometrics},
    number = {7},
    pages = {e156},
    title = {{Comparison and benchmark of name-to-gender inference services}},
    url = {https://doi.org/10.7717/peerj-cs.156},
    volume = {2018},
    year = {2018}
}

@misc{scipy_2001,
    note = {[Online; accessed ]},
    author = {Jones, Eric and Oliphant, Travis and Peterson, Pearu and Others},
    booktitle = {Http://Www.Scipy.Org/},
    title = {{SciPy: Open Source Scientific Tools for Python, 2001 (http://www.scipy.org/)}},
    url = {http://www.scipy.org/},
    year = {2015}
}

@article{garg2018word,
    abstract = {Word embeddings are a powerful machine-learning framework that represents each English word by a vector. The geometric relationship between these vectors captures meaningful semantic relationships between the corresponding words. In this paper, we develop a framework to demonstrate how the temporal dynamics of the embedding helps to quantify changes in stereotypes and attitudes toward women and ethnic minorities in the 20th and 21st centuries in the United States. We integrate word embeddings trained on 100 y of text data with the US Census to show that changes in the embedding track closely with demographic and occupation shifts over time. The embedding captures societal shifts—e.g., the women's movement in the 1960s and Asian immigration into the United States—and also illuminates how specific adjectives and occupations became more closely associated with certain populations over time. Our framework for temporal analysis of word embedding opens up a fruitful intersection between machine learning and quantitative social science.},
    author = {Garg, Nikhil and Schiebinger, Londa and Jurafsky, Dan and Zou, James},
    doi = {10.1073/pnas.1720347115},
    issn = {10916490},
    journal = {Proceedings of the National Academy of Sciences of the United States of America},
    keywords = {Ethnic stereotypes,Gender stereotypes,Word embedding},
    number = {16},
    pages = {E3635--E3644},
    publisher = {National Academy of Sciences},
    title = {{Word embeddings quantify 100 years of gender and ethnic stereotypes}},
    url = {https://www.pnas.org/content/115/16/E3635},
    volume = {115},
    year = {2018}
}

@inproceedings{rosenfeld2018deep,
    abstract = {A compilation of numerous studies of oxygen profiles in marine sediments and estimated fluxes across the sediment-water interface supports the existence of a simple relationship between oxygen penetration depth CL), benthic oxygen flux (I{\$}), and bottom water oxygen concentration([O,],,). This relationship is: where {\$}J and D, are the porosity and diffusivity of 0, in sediment, respectively. The relationship holds well for continental margin environments, but not in open ocean basins. The success of this simple relationship in describing continental margin field data suggests that the pore water profiles are at steady state, the depth distribution and reactivity of organic carbon is nearly uniform, and pore water irrigation is generally negligible. This work also demonstrates the consistency of the data recently collected by several independent works.},
    author = {Rosenfeld, Alex and Erk, Katrin},
    booktitle = {Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},
    doi = {10.18653/v1/n18-1044},
    pages = {474--484},
    title = {{Deep Neural Models of Semantic Shift}},
    volume = {1},
    year = {2018}
}

@article{srivastava2014dropout,
    author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
    journal = {The Journal of Machine Learning Research},
    number = {1},
    pages = {1929--1958},
    publisher = {JMLR. org},
    title = {{Dropout: a simple way to prevent neural networks from overfitting}},
    volume = {15},
    year = {2014}
}

@inproceedings{rosin2017learning,
    title = "Learning Word Relatedness over Time",
    author = "Rosin, Guy D.  and
      Adar, Eytan  and
      Radinsky, Kira",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D17-1121",
    doi = "10.18653/v1/D17-1121",
    pages = "1168--1178",
    abstract = "Search systems are often focused on providing relevant results for the {``}now{''}, assuming both corpora and user needs that focus on the present. However, many corpora today reflect significant longitudinal collections ranging from 20 years of the Web to hundreds of years of digitized newspapers and books. Understanding the temporal intent of the user and retrieving the most relevant historical content has become a significant challenge. Common search features, such as query expansion, leverage the relationship between terms but cannot function well across all times when relationships vary temporally. In this work, we introduce a temporal relationship model that is extracted from longitudinal data collections. The model supports the task of identifying, given two words, when they relate to each other. We present an algorithmic framework for this task and show its application for the task of query expansion, achieving high gain.",
}

@inproceedings{huang2017examining,
    abstract = {{\textcopyright} 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. Traditional data on influenza vaccination has several limitations: high cost, limited coverage of underrepresented groups, and low sensitivity to emerging public health issues. Social media, such as Twitter, provide an alternative way to understand a population's vaccination-related opinions and behaviors. In this study, we build and employ several natural language classifiers to examine and analyze behavioral patterns regarding influenza vaccination in Twitter across three dimensions: temporality (by week and month), geography (by US region), and demography (by gender). Our best results are highly correlated official government data, with a correlation over 0.90, providing validation of our approach. We then suggest a number of directions for future work.},
    author = {Huang, Xiaolei and Smith, Michael C. and Paul, Michael J. and Ryzhkov, Dmytro and Quinn, Sandra C. and Broniatowski, David A. and Dredze, Mark},
    booktitle = {Workshops at the Thirty-First AAAI Conference on Artificial Intelligence},
    isbn = {9781577357865},
    pages = {542--546},
    title = {{Examining patterns of influenza vaccination in social media}},
    volume = {WS-17-01},
    year = {2017}
}

@article{amrhein2003client,
    abstract = {Client language from a motivational interview (MI) and drug use outcome were investigated. Interview videotapes of 84 drug abusers were coded for frequency and strength of utterances expressing commitment, desire, ability, need, readiness, and reasons to change or maintain their habit. Cluster analysis of proportion days abstinent (PDA) revealed 3 groups: high PDA at intake and follow-up (3, 6, 9, 12 months; maintainers); low intake PDA/high follow-up PDA (changers); and low intake PDA/low to moderate follow-up PDA (stragglers). Distinct group patterns emerged for commitment strength (CS) during MI. Clients dishonest in checklist self-report exhibited CS similar to stragglers. CS for client evaluation of a change plan predicted outcome PDA. CS was predicted by strength of desire, ability, need, and reasons, but more strongly predicted outcome PDA, suggesting CS is a pathway for their influence on behavior.},
    author = {Amrhein, Paul C. and Miller, William R. and Yahne, Carolina E. and Palmer, Michael and Fulcher, Laura},
    doi = {10.1037/0022-006X.71.5.862},
    issn = {0022006X},
    journal = {Journal of Consulting and Clinical Psychology},
    number = {5},
    pages = {862--878},
    publisher = {American Psychological Association},
    title = {{Client commitment language during motivational interviewing predicts drug use outcomes}},
    volume = {71},
    year = {2003}
}

@article{rose2012face,
    abstract = {Social websites like Facebook enable users to upload self-created digital images; it is therefore of interest to see how gender is performed in this domain. A panel used a literature review of pictorial features associated with gender traits, and a sample of Facebook pictures to assess gender stereotypes present in Facebook images. Traits emerging in greater prominence in pictures of males included active, dominant, and independent. Those prominent with female users included attractive and dependent. These findings generally conform to gender stereotypes found in prior research and extend the research regarding stereotypical gender traits displayed in professional media depictions to self-selected social media displays. They also extend the research on gender differences in impression management generally, in both interpersonal communication and social media, to include gender-specific traits that are part of young mens and women's impression management. ABSTRACT FROM PUBLISHER]; Copyright of Communication Quarterly is the property of Eastern Communication Association and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)},
    author = {Rose, Jessica and Mackey-Kallis, Susan and Shyles, Len and Barry, Kelly and Biagini, Danielle and Hart, Colleen and Jack, Lauren},
    doi = {10.1080/01463373.2012.725005},
    issn = {01463373},
    journal = {Communication Quarterly},
    keywords = {Facebook,Gender Display,Impression Management,Role Theory,Social Media},
    number = {5},
    pages = {588--607},
    publisher = {Taylor {\&} Francis},
    title = {{Face it: The Impact of Gender on Social Media Images}},
    volume = {60},
    year = {2012}
}

@inproceedings{buolamwini2018gender,
    author = {Buolamwini, Joy and Gebru, Timnit},
    booktitle = {Conference on Fairness, Accountability and Transparency},
    pages = {77--91},
    title = {{Gender shades: Intersectional accuracy disparities in commercial gender classification}},
    year = {2018}
}

@inproceedings{diaz2018addressing,
    abstract = {Recent studies have identified various forms of bias in language-based models, raising concerns about the risk of propagating social biases against certain groups based on sociodemographic factors (e.g., gender, race, geography). In this study, we analyze the treatment of age-related terms across 15 sentiment analysis models and 10 widely-used GloVe word embeddings and attempt to alleviate bias through a method of processing model training data. Our results show significant age bias is encoded in the outputs of many sentiment analysis algorithms and word embeddings, and we can alleviate this bias by manipulating training data.},
    author = {D{\'{i}}az, Mark and Johnson, Isaac and Lazar, Amanda and Piper, Anne Marie and Gergle, Darren},
    booktitle = {Conference on Human Factors in Computing Systems - Proceedings},
    doi = {10.1145/3173574.3173986},
    isbn = {9781450356206},
    keywords = {Aging,Older adults, algorithmic bias,Sentiment analysis},
    pages = {412:1----412:14},
    title = {{Addressing age-related bias in sentiment analysis}},
    url = {http://doi.acm.org/10.1145/3173574.3173986},
    volume = {2018-April},
    year = {2018}
}

@inproceedings{liu2017adversarial,
    abstract = {Neural network models have shown their promising opportunities for multi-task learning, which focus on learning the shared layers to extract the common and task-invariant features. However, in most existing approaches, the extracted shared features are prone to be contaminated by task-specific features or the noise brought by other tasks. In this paper, we propose an adversarial multi-task learning framework, alleviating the shared and private latent feature spaces from interfering with each other. We conduct extensive experiments on 16 different text classification tasks, which demonstrates the benefits of our approach. Besides, we show that the shared knowledge learned by our proposed model can be regarded as off-the-shelf knowledge and easily transferred to new tasks. The datasets of all 16 tasks are publicly available at $\backslash$url{\{}http://nlp.fudan.edu.cn/data/{\}}},
    author = {Liu, Pengfei and Qiu, Xipeng and Huang, Xuanjing},
    booktitle = {ACL 2017 - 55th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference (Long Papers)},
    doi = {10.18653/v1/P17-1001},
    isbn = {9781945626753},
    pages = {1--10},
    title = {{Adversarial multi-task learning for text classification}},
    volume = {1},
    year = {2017}
}

@inproceedings{he2016ups,
    abstract = {Building a successful recommender system depends on understanding both the dimensions of people's preferences as well as their dynamics. In certain domains, such as fashion, modeling such preferences can be incredibly difficult, due to the need to simultaneously model the visual appearance of products as well as their evolution over time. The subtle semantics and non-linear dynamics of fashion evolution raise unique challenges especially considering the sparsity and large scale of the underlying datasets. In this paper we build novel models for the One-Class Collaborative Filtering setting, where our goal is to estimate users' fashion-aware personalized ranking functions based on their past feedback. To uncover the complex and evolving visual factors that people consider when evaluating products, our method combines high-level visual features extracted from a deep convolutional neural network, users' past feedback, as well as evolving trends within the community. Experimentally we evaluate our method on two large real-world datasets from Amazon.com, where we show it to outperform state-of-the-art personalized ranking measures, and also use it to visualize the high-level fashion trends across the 11-year span of our dataset.},
    author = {He, Ruining and McAuley, Julian},
    booktitle = {Proceedings of the 25th International Conference on World Wide Web (WWW)},
    doi = {10.1145/2872427.2883037},
    isbn = {9781450341431},
    keywords = {Fashion Evolution,Personalized Ranking,Recommender Systems,Visual Dimensions},
    organization = {International World Wide Web Conferences Steering Committee},
    pages = {507--517},
    title = {{Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering}},
    volume = {3},
    year = {2016}
}

@article{tieleman2012lecture,
    author = {Tieleman, Tijmen and Hinton, Geoffrey},
    journal = {COURSERA: Neural networks for machine learning},
    number = {2},
    pages = {26--31},
    title = {{Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude}},
    url = {https://www.cs.toronto.edu/{~}tijmen/csc321/slides/lecture{\_}slides{\_}lec6.pdf},
    volume = {4},
    year = {2012}
}

@incollection{liao2016analysing,
    abstract = {This paper intend to present an approach to analyse the change of word meaning based on word embedding, which is a more general method to quantize words than before. Through analysing the similar words and clustering in different period, semantic change could be detected. We analysed the trend of semantic change through density clustering method called DBSCAN. Statics and data visualization is also included to make the result more clear. Some words like `gay', `mouse' are traced as case to prove this approach works. At last, we also compared the context words and similar words on semantic presentation and proved the context words worked better.},
    author = {Liao, Xuanyi and Cheng, Guang},
    booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
    doi = {10.1007/978-3-319-50496-4_18},
    issn = {16113349},
    keywords = {Google books N-gram corpus,Semantic change,Word embedding,Word similarity},
    pages = {213--223},
    publisher = {Springer},
    title = {{Analysing the semantic change based on word embedding}},
    volume = {10102},
    year = {2016}
}

@inproceedings{sogaard2014s,
    title = "What{'}s in a p-value in {NLP}?",
    author = "S{\o}gaard, Anders  and
      Johannsen, Anders  and
      Plank, Barbara  and
      Hovy, Dirk  and
      Mart{\'\i}nez Alonso, Hector",
    booktitle = "Proceedings of the Eighteenth Conference on Computational Natural Language Learning",
    month = jun,
    year = "2014",
    address = "Ann Arbor, Michigan",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W14-1601",
    doi = "10.3115/v1/W14-1601",
    pages = "1--10",
}

@misc{yelp_2019,
    author = {Yelp},
    booktitle = {Yelp},
    month = {feb},
    publisher = {Yelp},
    title = {{Yelp Dataset Challenge}},
    url = {https://www.yelp.com/dataset/challenge},
    year = {2018}
}

@inproceedings{founta2018large,
    abstract = {In recent years, offensive, abusive and hateful language, sexism, racism and other types of aggressive and cyberbullying behavior have been manifesting with increased frequency, and in many online social media platforms. In fact, past scientific work focused on studying these forms in popular media, such as Facebook and Twitter. Building on such work, we present an 8-month study of the various forms of abusive behavior on Twitter, in a holistic fashion. Departing from past work, we examine a wide variety of labeling schemes, which cover different forms of abusive behavior, at the same time. We propose an incremental and iterative methodology, that utilizes the power of crowdsourcing to annotate a large scale collection of tweets with a set of abuse-related labels. In fact, by applying our methodology including statistical analysis for label merging or elimination, we identify a reduced but robust set of labels. Finally, we offer a first overview and findings of our collected and annotated dataset of 100 thousand tweets, which we make publicly available for further scientific exploration.},
    author = {Founta, Antigoni Maria and Djouvas, Constantinos and Chatzakou, Despoina and Leontiadis, Ilias and Blackburn, Jeremy and Stringhini, Gianluca and Vakali, Athena and Sirivianos, Michael and Kourtellis, Nicolas},
    booktitle = {12th International AAAI Conference on Web and Social Media, ICWSM 2018},
    isbn = {9781577357988},
    pages = {491--500},
    title = {{Large scale crowdsourcing and characterization of twitter abusive behavior}},
    url = {https://www.aaai.org/ocs/index.php/ICWSM/ICWSM18/paper/view/17909/17041},
    year = {2018}
}

@inproceedings{li2018towards,
    abstract = {Written text often provides sufficient clues to identify the author, their gender, age, and other important attributes. Consequently, the authorship of training and evaluation corpora can have unforeseen impacts, including differing model performance for different user groups, as well as privacy implications. In this paper, we propose an approach to explicitly obscure important author characteristics at training time, such that representations learned are invariant to these attributes. Evaluating on two tasks, we show that this leads to increased privacy in the learned representations, as well as more robust models to varying evaluation conditions, including out-of-domain corpora.},
    author = {Li, Yitong and Baldwin, Timothy and Cohn, Trevor},
    booktitle = {ACL 2018 - 56th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference (Long Papers)},
    isbn = {9781948087346},
    pages = {25--30},
    title = {{Towards robust and privacy-preserving text representations}},
    volume = {2},
    year = {2018}
}

@inproceedings{gulcehre2016noisy,
    abstract = {Common nonlinear activation functions used in neural networks can cause training difficulties due to the saturation behavior of the activation function, which may hide dependencies that are not visible to vanilla-SGD (using first order gradients only). Gating mechanisms that use softly saturating activation functions to emulate the discrete switching of digital logic circuits are good examples of this. We propose to exploit the injection of appropriate noise so that the gradients may flow easily, even if the noiseless application of the activation function would yield zero gradient. Large noise will dominate the noise-free gradient and allow stochastic gradient descent toexplore more. By adding noise only to the problematic parts of the activation function, we allow the optimization procedure to explore the boundary between the degenerate (saturating) and the well-behaved parts of the activation function. We also establish connections to simulated annealing, when the amount of noise is annealed down, making it easier to optimize hard objective functions. We find experimentally that replacing such saturating activation functions by noisy variants helps training in many contexts, yielding state-of-the-art or competitive results on different datasets and task, especially when training seems to be the most difficult, e.g., when curriculum learning is necessary to obtain good results.},
    author = {Gulcehre, Caglar and Moczulski, Marcin and Denil, Misha and Bengio, Yoshua},
    booktitle = {33rd International Conference on Machine Learning, ICML 2016},
    isbn = {9781510829008},
    pages = {4457--4466},
    title = {{Noisy activation functions}},
    volume = {6},
    year = {2016}
}

@inproceedings{ribeiro2018characterizing,
    abstract = {Most current approaches to characterize and detect hate speech focus on $\backslash$textit{\{}content{\}} posted in Online Social Networks. They face shortcomings to collect and annotate hateful speech due to the incompleteness and noisiness of OSN text and the subjectivity of hate speech. These limitations are often aided with constraints that oversimplify the problem, such as considering only tweets containing hate-related words. In this work we partially address these issues by shifting the focus towards $\backslash$textit{\{}users{\}}. We develop and employ a robust methodology to collect and annotate hateful users which does not depend directly on lexicon and where the users are annotated given their entire profile. This results in a sample of Twitter's retweet graph containing {\$}100,386{\$} users, out of which {\$}4,972{\$} were annotated. We also collect the users who were banned in the three months that followed the data collection. We show that hateful users differ from normal ones in terms of their activity patterns, word usage and as well as network structure. We obtain similar results comparing the neighbors of hateful vs. neighbors of normal users and also suspended users vs. active users, increasing the robustness of our analysis. We observe that hateful users are densely connected, and thus formulate the hate speech detection problem as a task of semi-supervised learning over a graph, exploiting the network of connections on Twitter. We find that a node embedding algorithm, which exploits the graph structure, outperforms content-based approaches for the detection of both hateful ({\$}95\backslash{\%}{\$} AUC vs {\$}88\backslash{\%}{\$} AUC) and suspended users ({\$}93\backslash{\%}{\$} AUC vs {\$}88\backslash{\%}{\$} AUC). Altogether, we present a user-centric view of hate speech, paving the way for better detection and understanding of this relevant and challenging issue.},
    author = {Ribeiro, Manoel Horta and Calais, Pedro H. and Santos, Yuri A. and Almeida, Virg{\'{i}}lio A.F. and Meira, Wagner},
    booktitle = {12th International AAAI Conference on Web and Social Media, ICWSM 2018},
    isbn = {9781577357988},
    pages = {676--679},
    title = {{Characterizing and detecting hateful users on twitter}},
    url = {https://www.aaai.org/ocs/index.php/ICWSM/ICWSM18/paper/view/17837/17079},
    year = {2018}
}

@inproceedings{bamler2017dynamic,
    abstract = {We present a probabilistic language model for time-stamped text data which tracks the semantic evolution of individual words over time. The model represents words and contexts by latent trajectories in an embedding space. At each moment in time, the embedding vectors are inferred from a probabilistic version of word2vec [Mikolov et al., 2013]. These embedding vectors are connected in time through a latent diffusion process. We describe two scalable variational inference algorithms--skip-gram smoothing and skip-gram filtering--that allow us to train the model jointly over all times; thus learning on all data while simultaneously allowing word and context vectors to drift. Experimental results on three different corpora demonstrate that our dynamic model infers word embedding trajectories that are more interpretable and lead to higher predictive likelihoods than competing methods that are based on static models trained separately on time slices.},
    author = {Bamler, Robert and Mandt, Stephan},
    booktitle = {34th International Conference on Machine Learning, ICML 2017},
    isbn = {9781510855144},
    organization = {JMLR. org},
    pages = {607--621},
    title = {{Dynamic word embeddings}},
    volume = {1},
    year = {2017}
}

@inproceedings{mikolov2013distributed,
    abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
    author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
    title = {Distributed Representations of Words and Phrases and Their Compositionality},
    booktitle = {Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 2},
    series = {NIPS'13},
    year = {2013},
    location = {Lake Tahoe, Nevada},
    pages = {3111--3119},
    numpages = {9},
    url = {http://dl.acm.org/citation.cfm?id=2999792.2999959},
    acmid = {2999959},
    publisher = {Curran Associates Inc.},
    address = {USA},
} 

@article{Mohammad13,
    abstract = {Even though considerable attention has been given to the polarity of words (positive and negative) and the creation of large polarity lexicons, research in emotion analysis has had to rely on limited and small emotion lexicons. In this paper we show how the combined strength and wisdom of the crowds can be used to generate a large, high-quality, word-emotion and word-polarity association lexicon quickly and inexpensively. We enumerate the challenges in emotion annotation in a crowdsourcing scenario and propose solutions to address them. Most notably, in addition to questions about emotions associated with terms, we show how the inclusion of a word choice question can discourage malicious data entry, help identify instances where the annotator may not be familiar with the target term (allowing us to reject such annotations), and help obtain annotations at sense level (rather than at word level). We conducted experiments on how to formulate the emotion-annotation questions, and show that asking if a term is associated with an emotion leads to markedly higher inter-annotator agreement than that obtained by asking if a term evokes an emotion.},
    author = {Mohammad, Saif M. and Turney, Peter D.},
    doi = {10.1111/j.1467-8640.2012.00460.x},
    issn = {08247935},
    journal = {Computational Intelligence},
    keywords = {Mechanical Turk,affect,crowdsourcing,emotion lexicon,emotions,polarity,polarity lexicon,semantic orientation,sentiment analysis,word-emotion associations},
    number = {3},
    pages = {436--465},
    publisher = {Wiley Online Library},
    title = {{Crowdsourcing a word-emotion association lexicon}},
    volume = {29},
    year = {2013}
}

@inproceedings{perez2017predicting,
    abstract = {{\textcopyright} 2017 Association for Computational Linguistics. As the number of people receiving psychotherapeutic treatment increases, the automatic evaluation of counseling practice arises as an important challenge in the clinical domain. In this paper, we address the automatic evaluation of counseling performance by analyzing counselors' language during their interaction with clients. In particular, we present a model towards the automation of Motivational Interviewing (MI) coding, which is the current gold standard to evaluate MI counseling. First, we build a dataset of hand labeled MI encounters; second, we use text-based methods to extract and analyze linguistic patterns associated with counselor behaviors; and third, we develop an automatic system to predict these behaviors. We introduce a new set of features based on semantic information and syntactic patterns, and show that they lead to accuracy figures of up to 90{\%}, which represent a significant improvement with respect to features used in the past.},
    author = {P{\'{e}}rez-Rosas, Ver{\'{o}}nica and Mihalcea, Rada and Resnicow, Kenneth and Singh, Satinder and An, Lawrence and Goggin, Kathy J. and Catley, Delwyn},
    booktitle = {15th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2017 - Proceedings of Conference},
    isbn = {9781510838604},
    pages = {1128--1137},
    title = {{Predicting counselor behaviors in motivational interviewing encounters}},
    volume = {1},
    year = {2017}
}

@inproceedings{rosenthal2011age,
    abstract = {We investigate whether wording, stylistic choices, and online behavior can be used to predict the age category of blog authors. Our hypothesis is that significant changes in writing style distinguish pre-social me- dia bloggers from post-social media blog- gers. Through experimentation with a range of years, we found that the birth dates of students in college at the time when social media such as AIM, SMS text messaging, MySpace and Facebook first became popular, enable accurate age pre- diction. We also show that internet writing characteristics are important features for age prediction, but that lexical content is also needed to produce significantly more accurate results. Our best results allow for 81.57{\%} accuracy.},
    author = {Rosenthal, Sara and McKeown, Kathleen},
    booktitle = {ACL-HLT 2011 - Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},
    isbn = {9781932432879},
    organization = {Association for Computational Linguistics},
    pages = {763--772},
    title = {{Age prediction in blogs: A study of style, content, and online behavior in pre- and post-social media generations}},
    volume = {1},
    year = {2011}
}

@inproceedings{bolukbasi2016man,
    author = {Bolukbasi, Tolga and Chang, Kai-Wei and Zou, James Y and Saligrama, Venkatesh and Kalai, Adam T},
    booktitle = {Advances in Neural Information Processing Systems},
    pages = {4349--4357},
    title = {{Man is to computer programmer as woman is to homemaker? debiasing word embeddings}},
    year = {2016}
}

@inproceedings{park2018reducing,
    abstract = {Abusive language detection models tend to have a problem of being biased toward identity words of a certain group of people because of imbalanced training datasets. For example, "You are a good woman" was considered "sexist" when trained on an existing dataset. Such model bias is an obstacle for models to be robust enough for practical use. In this work, we measure gender biases on models trained with different abusive language datasets, while analyzing the effect of different pre-trained word embeddings and model architectures. We also experiment with three bias mitigation methods: (1) debiased word embeddings, (2) gender swap data augmentation, and (3) fine-tuning with a larger corpus. These methods can effectively reduce gender bias by 90-98{\%} and can be extended to correct model bias in other scenarios.},
    author = {Park, Ji Ho and Shin, Jamin and Fung, Pascale},
    booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
    doi = {10.18653/v1/d18-1302},
    pages = {2799--2804},
    title = {{Reducing Gender Bias in Abusive Language Detection}},
    url = {https://aclweb.org/anthology/D18-1302},
    year = {2019}
}

@misc{hinton_srivastava_swersky,
    abstract = {Tieleman, Tijmen and Hinton, Geoffrey. Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural Networks for Machine Learning, 4, 2012},
    author = {Hinton, Geoffrey E. and Srivastava, Nitish and Swersky, Kevin},
    booktitle = {COURSERA: Neural Networks for Machine Learning},
    pages = {31},
    title = {{Lecture 6a- overview of mini-batch gradient descent}},
    url = {http://www.cs.toronto.edu/{~}tijmen/csc321/slides/lecture{\_}slides{\_}lec6.pdf},
    year = {2012}
}

@inproceedings{dixon2018measuring,
    author = {Dixon, Lucas and Li, John and Sorensen, Jeffrey and Thain, Nithum and Vasserman, Lucy},
    title = {Measuring and Mitigating Unintended Bias in Text Classification},
    booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
    series = {AIES '18},
    year = {2018},
    isbn = {978-1-4503-6012-8},
    location = {New Orleans, LA, USA},
    pages = {67--73},
    numpages = {7},
    url = {http://doi.acm.org/10.1145/3278721.3278729},
    doi = {10.1145/3278721.3278729},
    acmid = {3278729},
    publisher = {ACM},
    address = {New York, NY, USA},
    keywords = {algorithmic bias, fairness, machine learning, natural language processing, text classification},
} 

@inproceedings{hovy2015tagging,
    title = "Tagging Performance Correlates with Author Age",
    author = "Hovy, Dirk  and
      S{\o}gaard, Anders",
    booktitle = "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)",
    month = jul,
    year = "2015",
    address = "Beijing, China",
    publisher = "Association for Computational Linguistics",
    doi = "10.3115/v1/P15-2079",
    pages = "483--488",
}

@inproceedings{zhao2017men,
    title = "Men Also Like Shopping: Reducing Gender Bias Amplification using Corpus-level Constraints",
    author = "Zhao, Jieyu  and
      Wang, Tianlu  and
      Yatskar, Mark  and
      Ordonez, Vicente  and
      Chang, Kai-Wei",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D17-1323",
    doi = "10.18653/v1/D17-1323",
    pages = "2979--2989",
    abstract = "Language is increasingly being used to de-fine rich visual recognition problems with supporting image collections sourced from the web. Structured prediction models are used in these tasks to take advantage of correlations between co-occurring labels and visual input but risk inadvertently encoding social biases found in web corpora. In this work, we study data and models associated with multilabel object classification and visual semantic role labeling. We find that (a) datasets for these tasks contain significant gender bias and (b) models trained on these datasets further amplify existing bias. For example, the activity cooking is over 33{\%} more likely to involve females than males in a training set, and a trained model further amplifies the disparity to 68{\%} at test time. We propose to inject corpus-level constraints for calibrating existing structured prediction models and design an algorithm based on Lagrangian relaxation for collective inference. Our method results in almost no performance loss for the underlying recognition task but decreases the magnitude of bias amplification by 47.5{\%} and 40.5{\%} for multilabel classification and visual semantic role labeling, respectivelyã€‚",
}

@misc{figure_eight_2015,
    abstract = {Around the world and in every discipline, data sharing is on the advance. The online archive PsychData shows just how great the benefits can be},
    author = {Weichselgartner, Erich and Winkler-Nees, Stefan},
    booktitle = {German Research},
    doi = {10.1002/germ.201190003},
    number = {3},
    pages = {19--19},
    publisher = {Figure Eight Inc.},
    title = {{Data for Everyone!}},
    url = {https://www.crowdflower.com/data-for-everyone/},
    volume = {32},
    year = {2010}
}

@inproceedings{W18-5904,
    abstract = {The volume of data encapsulated within social media continues to grow, and, consequently, there is a growing interest in developing effective systems that can convert this data into usable knowledge. Over recent years, initiatives have been taken to enable and promote the utilization of knowledge derived from social media to perform health related tasks. These initiatives include the development of data mining systems and the preparation of datasets that can be used to train such systems. The overarching focus of the SMM4H shared tasks is to release annotated social media based health related datasets to the research community, and to compare the performances of distinct natural language processing and machine learning systems on tasks involving these datasets. The second execution of the SMM4H shared tasks comprised of three subtasks involving annotated user posts from Twitter (tweets): (i) automatic classification of tweets mentioning an adverse drug reaction (ADR) (ii) automatic classification of tweets containing reports of first-person medication intake, and (iii) automatic normalization of ADR mentions to MedDRA concepts. A total of 15 teams participated and 55 system runs were submitted. The best performing systems for tasks 2 and 3 outperformed the current state of the art systems.},
    author = {Sarker, Abeed and Gonzalez-Hernandez, Graciela},
    booktitle = {CEUR Workshop Proceedings},
    issn = {16130073},
    pages = {43--48},
    publisher = {Association for Computational Linguistics},
    title = {{Overview of the second social media mining for health (SMM4H) shared tasks at AMIA 2017}},
    url = {http://aclweb.org/anthology/W18-5904},
    volume = {1996},
    year = {2017}
}

@article{borsari2015session,
    abstract = {{\textcopyright} 2017 American Psychological Association. Objective: Brief motivational interventions (BMIs) are currently the most efficacious individual intervention for mandated college students. However, little is known about how BMIs facilitate client language in relation to subsequent changes in alcohol use and problems in mandated student samples. Method: The current study used the Motivational Interviewing Skills Code (MISC 2.0; Miller, Moyers, Ernst, {\&} Amrhein, 2003) to code BMI sessions (N = 252) from 2 randomized clinical trials that led to significant reductions in alcohol use and alcohol-related problems in mandated student drinkers. A proportion of change language was calculated for each decile (1st to 10th) of the BMI sessions. Results: Latent class analyses of in-session speech indicated that there were 3 distinct trajectories of change language over the course of the session: high (n = 59), increasing (n = 122), and low (n = 71). Members of the high trajectory group showed higher rates of alcohol-related problems prior to the BMI and members of the low trajectory group were more likely to be male. Six months following the BMI, members of the high and low trajectory groups demonstrated significant reductions in alcohol use and problems, and members of the increasing trajectory group only reduced alcohol-related problems. Conclusions: Associations among the 3 trajectories of client change language and subsequent reductions in alcohol use and problems partially supported the technical hypothesis of MI efficacy. Client factors as well as the nature of the discussion of personalized feedback may determine the link between in-session client language and subsequent behavior change. (PsycINFO Database Record (c) 2018 APA, all rights reserved).},
    author = {Borsari, Brian and Apodaca, Timothy R. and Jackson, Kristina M. and Fernandez, Anne and Mastroleo, Nadine R. and Magill, Molly and Barnett, Nancy P. and Carey, Kate B.},
    doi = {10.1037/ccp0000255},
    issn = {19392117},
    journal = {Journal of Consulting and Clinical Psychology},
    keywords = {alcohol use,brief intervention,mandated students,motivational interviewing,therapy process},
    number = {2},
    pages = {158--168},
    publisher = {American Psychological Association},
    title = {{Trajectories of in-session change language in brief motivational interventions with mandated college students.}},
    volume = {86},
    year = {2018}
}

@inproceedings{hovy2015demographic,
    title = "Demographic Factors Improve Classification Performance",
    author = "Hovy, Dirk",
    booktitle = "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = jul,
    year = "2015",
    address = "Beijing, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P15-1073",
    doi = "10.3115/v1/P15-1073",
    pages = "752--762",
}

@inproceedings{volkova2013exploring,
    abstract = {Different demographics, e.g., gender or age, can demonstrate substantial variation in their language use, particularly in informal contexts such as social media. In this paper we focus on learning gender differences in the use of sub- jective language in English, Spanish, and Rus- sian Twitter data, and explore cross-cultural differences in emoticon and hashtag use for male and female users. We show that gen- der differences in subjective language can ef- fectively be used to improve sentiment anal- ysis, and in particular, polarity classification for Spanish and Russian. Our results show statistically significant relative F-measure im- provement over the gender-independent base- line 1.5{\%} and 1{\%} for Russian, 2{\%} and 0.5{\%} for Spanish, and 2.5{\%} and 5{\%} for English for polarity and subjectivity classificatio},
    address = {Seattle, Washington, USA},
    author = {Volkova, Svitlana and Wilson, Theresa and Yarowsky, David},
    booktitle = {EMNLP 2013 - 2013 Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference},
    isbn = {9781937284978},
    pages = {1815--1827},
    publisher = {Association for Computational Linguistics},
    title = {{Exploring demographic language variations to improve multilingual sentiment analysis in social media}},
    url = {http://aclweb.org/anthology//D/D13/D13-1187.pdf},
    year = {2013}
}

@inproceedings{blitzer-07,
    abstract = {Automatic sentiment classification has been extensively studied and applied in recent years. However, sentiment is expressed differently in different domains, and annotating corpora for every possible domain of interest is impractical. We investigate domain adaptation for sentiment classifiers, focusing on online reviews for different types of products. First, we extend to sentiment classification the recently-proposed structural correspondence learning (SCL) algorithm, reducing the relative error due to adaptation between domains by an average of 30{\%} over the original SCL algorithm and 46{\%} over a supervised baseline. Second, we identify a measure of domain similarity that correlates well with the potential for adaptation of a classifier from one domain to another. This measure could for instance be used to select a small set of domains to annotate whose trained classifiers would transfer well to many other domains. {\textcopyright} 2007 Association for Computational Linguistics.},
    author = {Blitzer, John and Dredze, Mark and Pereira, Fernando},
    booktitle = {ACL 2007 - Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics},
    isbn = {9781932432862},
    pages = {440--447},
    title = {{Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification}},
    year = {2007}
}

@inproceedings{dredze2013carmen,
    abstract = {Public health applications using social media often require accurate, broad-coverage location information. However, the standard information provided by social media APIs, such as Twitter, cover a limited number of messages. This paper presents Carmen, a geolocation system that can determine structured location information for messages provided by the Twitter API. Our system utilizes geocoding tools and a combination of automatic and manual alias resolution methods to infer location structures from GPS positions and user-provided profile data. We show that our system is accurate and covers many locations, and we demonstrate its utility for improving influenza surveillance. Copyright {\textcopyright} 2013, Association for the Advancement of Artificial Intelligence. All rights reserved.},
    author = {Dredze, Mark and Paul, Michael J. and Bergsma, Shane and Tran, Hieu},
    booktitle = {AAAI Workshop - Technical Report},
    isbn = {9781577356202},
    pages = {20--24},
    title = {{Carmen: A twitter geolocation system with applications to public health}},
    volume = {WS-13-09},
    year = {2013}
}

@inproceedings{zhang2014explicit,
    abstract = {Collaborative Filtering(CF)-based recommendation algorithms, such as Latent Factor Models (LFM), work well in terms of prediction accuracy. However, the latent features make it difficulty to explain the recommendation results to the users. Fortunately, with the continuous growth of online user reviews, the information available for training a recommender system is no longer limited to just numerical star ratings or user/item features. By extracting explicit user opinions about various aspects of a product from the reviews, it is possible to learn more details about what aspects a user cares, which further sheds light on the possibility to make explainable recommendations. In this work, we propose the Explicit Factor Model (EFM) to generate explainable recommendations, meanwhile keep a high prediction accuracy. We first extract explicit product features (i.e. aspects) and user opinions by phrase-level sentiment analysis on user reviews, then generate both recommendations and disrecommendations according to the specific product features to the user's interests and the hidden features learned. Besides, intuitional feature-level explanations about why an item is or is not recommended are generated from the model. Offline experimental results on several real-world datasets demonstrate the advantages of our framework over competitive baseline algorithms on both rating prediction and top-K recommendation tasks. Online experiments show that the detailed explanations make the recommendations and disrecommendations more inuential on user's purchasing behavior Copyright 2014 ACM.},
    author = {Zhang, Yongfeng and Lai, Guokun and Zhang, Min and Zhang, Yi and Liu, Yiqun and Ma, Shaoping},
    booktitle = {SIGIR 2014 - Proceedings of the 37th International ACM SIGIR Conference on Research and Development in Information Retrieval},
    doi = {10.1145/2600428.2609579},
    isbn = {9781450322591},
    keywords = {Collaborative Filtering,Recommendation explanation,Recommender systems,Sentiment analysis},
    organization = {ACM},
    pages = {83--92},
    title = {{Explicit Factor Models for explainable recommendation based on phrase-level sentiment analysis}},
    year = {2014}
}

@article{eisenstein2014diffusion,
    abstract = {Computer-mediated communication is driving fundamental changes in the nature of written language. We investigate these changes by statistical analysis of a dataset comprising 107 million Twitter messages (authored by 2.7 million unique user accounts). Using a latent vector autoregressive model to aggregate across thousands of words, we identify high-level patterns in diffusion of linguistic change over the United States. Our model is robust to unpredictable changes in Twitter's sampling rate, and provides a probabilistic characterization of the relationship of macro-scale linguistic influence to a set of demographic and geographic predictors. The results of this analysis offer support for prior arguments that focus on geographical proximity and population size. However, demographic similarity -- especially with regard to race -- plays an even more central role, as cities with similar racial demographics are far more likely to share linguistic influence. Rather than moving towards a single unified "netspeak" dialect, language evolution in computer-mediated communication reproduces existing fault lines in spoken American English.},
    author = {Eisenstein, Jacob and O'Connor, Brendan and Smith, Noah A. and Xing, Eric P.},
    doi = {10.1371/journal.pone.0113114},
    issn = {19326203},
    journal = {PLoS ONE},
    month = {nov},
    number = {11},
    title = {{Diffusion of lexical change in social media}},
    volume = {9},
    year = {2014}
}

@inproceedings{kim2014convolutional,
    abstract = {We report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vectors for sentence-level classification tasks. We show that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks. Learning task-specific vectors through fine-tuning offers further gains in performance. We additionally propose a simple modification to the architecture to allow for the use of both task-specific and static vectors. The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks, which include sentiment analysis and question classification.},
    address = {Doha, Qatar},
    author = {Kim, Yoon},
    booktitle = {EMNLP 2014 - 2014 Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference},
    isbn = {9781937284961},
    pages = {1746--1751},
    publisher = {Association for Computational Linguistics},
    title = {{Convolutional neural networks for sentence classification}},
    url = {http://aclweb.org/anthology/D/D14/D14-1181.pdf},
    year = {2014}
    }
    
@article{hinds2018demographic,
    author = {Hinds, Joanne AND Joinson, Adam N.},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {What demographic attributes do our digital footprints reveal? A systematic review},
    year = {2018},
    month = {11},
    volume = {13},
    url = {https://doi.org/10.1371/journal.pone.0207112},
    pages = {1-40},
    abstract = {To what extent does our online activity reveal who we are? Recent research has demonstrated that the digital traces left by individuals as they browse and interact with others online may reveal who they are and what their interests may be. In the present paper we report a systematic review that synthesises current evidence on predicting demographic attributes from online digital traces. Studies were included if they met the following criteria: (i) they reported findings where at least one demographic attribute was predicted/inferred from at least one form of digital footprint, (ii) the method of prediction was automated, and (iii) the traces were either visible (e.g. tweets) or non-visible (e.g. clickstreams). We identified 327 studies published up until October 2018. Across these articles, 14 demographic attributes were successfully inferred from digital traces; the most studied included gender, age, location, and political orientation. For each of the demographic attributes identified, we provide a database containing the platforms and digital traces examined, sample sizes, accuracy measures and the classification methods applied. Finally, we discuss the main research trends/findings, methodological approaches and recommend directions for future research.},
    number = {11},
    doi = {10.1371/journal.pone.0207112}
}

@article{pennebaker2001linguistic,
    abstract = {Language, whether spoken or written, is an important window into people's emotional and cognitive worlds. Text analysis of these narratives, focusing on specific words or classes of words, has been used in numerous research studies including studies of emotional, cognitive, structural, and process components of individuals' verbal and written language. It was in this research context that the LIWC program was developed. The program analyzes text files on a word-by-word basis, calculating percentage words that match each of several language dimensions. Its output is a text file that can be opened in any of a variety of applications, including word processors and spreadsheet programs. The program has 68 pre-set dimensions (output variables) including linguistic dimensions, word categories tapping psychological constructs, and personal concern categories, and can accommodate user-defined dimensions as well. Easy to install and use, this software offers researchers in social, personality, clinical, and applied psychology a valuable tool for quantifying the rich but often slippery data provided in the form of personal narratives. The software comes complete on one diskette and runs on any Windows-based computer.},
    author = {Chung, Cindy K. and Pennebaker, James W.},
    doi = {10.4018/978-1-60960-741-8.ch012},
    journal = {Applied Natural Language Processing},
    number = {2001},
    pages = {206--229},
    title = {{Linguistic Inquiry and Word Count (LIWC)}},
    volume = {71},
    year = {2013}
}

@inproceedings{preoctiuc2018user,
    abstract = {User demographic inference from social media text has the potential to improve a range of downstream applications, including real-time passive polling or quantifying demographic bias. This study focuses on developing models for user-level race and ethnicity prediction. We introduce a data set of users who self-report their race/ethnicity through a survey, in contrast to previous approaches that use distantly supervised data or perceived labels. We develop predictive models from text which accurately predict the membership of a user to the four largest racial and ethnic groups with up to .884 AUC and make these available to the research community},
    author = {Preot, Daniel and Ungar, Lyle},
    booktitle = {Proceedings of the 27th International Conference on Computational Linguistics},
    pages = {1534--1545},
    title = {{User-Level Race and Ethnicity Predictors from Twitter Text}},
    url = {https://www.aclweb.org/anthology/C18-1130},
    year = {2018}
}

@article{pedregosa2011scikit,
    abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.org.},
    author = {Pedregosa, Fabian and Varoquaux, Gael and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, {\'{E}}douard},
    issn = {15324435},
    journal = {Journal of Machine Learning Research},
    keywords = {Model selection,Python,Supervised learning,Unsupervised learning},
    number = {Oct},
    pages = {2825--2830},
    title = {{Scikit-learn: Machine learning in Python}},
    volume = {12},
    year = {2011}
}

@article{ganin2016domain,
    author = {Ganin, Yaroslav and Ustinova, Evgeniya and Ajakan, Hana and Germain, Pascal and Larochelle, Hugo and Laviolette, Fran{\c{c}}ois and Marchand, Mario and Lempitsky, Victor},
    journal = {The Journal of Machine Learning Research},
    number = {1},
    pages = {2030--2096},
    publisher = {JMLR. org},
    title = {{Domain-adversarial training of neural networks}},
    volume = {17},
    year = {2016},
    url = {http://jmlr.org/papers/volume17/15-239/15-239.pdf}
}

@inproceedings{dubossarsky2017outta,
    title = "Outta Control: Laws of Semantic Change and Inherent Biases in Word Representation Models",
    author = "Dubossarsky, Haim  and
      Weinshall, Daphna  and
      Grossman, Eitan",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D17-1118",
    doi = "10.18653/v1/D17-1118",
    pages = "1136--1145",
    abstract = "This article evaluates three proposed laws of semantic change. Our claim is that in order to validate a putative law of semantic change, the effect should be observed in the genuine condition but absent or reduced in a suitably matched control condition, in which no change can possibly have taken place. Our analysis shows that the effects reported in recent literature must be substantially revised: (i) the proposed negative correlation between meaning change and word frequency is shown to be largely an artefact of the models of word representation used; (ii) the proposed negative correlation between meaning change and prototypicality is shown to be much weaker than what has been claimed in prior art; and (iii) the proposed positive correlation between meaning change and polysemy is largely an artefact of word frequency. These empirical observations are corroborated by analytical proofs that show that count representations introduce an inherent dependence on word frequency, and thus word frequency cannot be evaluated as an independent factor with these representations.",
}

@misc{yelp_2018,
    author = {Yelp},
    booktitle = {Yelp},
    month = {sep},
    title = {{An Introduction to Yelp Metrics as of June 30, 2016}},
    url = {https://www.yelp.com/factsheet},
    year = {2016}
}

@inproceedings{kong2014dependency,
    title = "A Dependency Parser for Tweets",
    author = "Kong, Lingpeng  and
      Schneider, Nathan  and
      Swayamdipta, Swabha  and
      Bhatia, Archna  and
      Dyer, Chris  and
      Smith, Noah A.",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D14-1108",
    doi = "10.3115/v1/D14-1108",
    pages = "1001--1012",
}

@inproceedings{wulczyn2017ex,
    abstract = {The damage personal attacks cause to online discourse motivates many platforms to try to curb the phenomenon. However, understanding the prevalence and impact of personal attacks in online platforms at scale remains surprisingly difficult. The contribution of this paper is to develop and illustrate a method that combines crowdsourcing and machine learning to analyze personal attacks at scale. We show an evaluation method for a classifier in terms of the aggregated number of crowd-workers it can approximate. We apply our methodology to English Wikipedia, generating a corpus of over 100k high quality human-labeled comments and 63M machine-labeled ones from a classifier that is as good as the aggregate of 3 crowd-workers, as measured by the area under the ROC curve and Spearman correlation. Using this corpus of machine-labeled scores, our methodology allows us to explore some of the open questions about the nature of online personal attacks. This reveals that the majority of personal attacks on Wikipedia are not the result of a few malicious users, nor primarily the consequence of allowing anonymous contributions from unregistered users.},
    author = {Wulczyn, Ellery and Thain, Nithum and Dixon, Lucas},
    booktitle = {26th International World Wide Web Conference, WWW 2017},
    doi = {10.1145/3038912.3052591},
    isbn = {9781450349130},
    pages = {1391--1399},
    title = {{Ex machina: Personal attacks seen at scale}},
    url = {https://dl.acm.org/citation.cfm?id=3038912.3052591},
    year = {2017}
}

@inproceedings{kingma2014adam,
    abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
    archivePrefix = {arXiv},
    arxivId = {1412.6980},
    author = {Kingma, Diederik P. and Ba, Jimmy},
    booktitle = {Proceedings of the 3rd International Conference on Learning Representations (ICLR)},
    eprint = {1412.6980},
    title = {{Adam: A Method for Stochastic Optimization}},
    url = {http://arxiv.org/abs/1412.6980},
    year = {2014}
}

@inproceedings{manning-EtAl:2014:P14-5,
    title = "The {S}tanford {C}ore{NLP} Natural Language Processing Toolkit",
    author = "Manning, Christopher  and
      Surdeanu, Mihai  and
      Bauer, John  and
      Finkel, Jenny  and
      Bethard, Steven  and
      McClosky, David",
    booktitle = "Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations",
    month = jun,
    year = "2014",
    address = "Baltimore, Maryland",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P14-5010",
    doi = "10.3115/v1/P14-5010",
    pages = "55--60",
}

@inproceedings{ICWSM1817809,
    abstract = {Moderators are believed to play a crucial role in ensuring the quality of discussion in online political debate forums.  The line between moderation and illegitimate censorship, where certain views or individuals are unfairly suppressed, however, is often difficult to define.  To better understand the relationship between moderation and censorship, we investigate whether users' perception of moderator bias is supported by how moderators act, using the Big Issues Debate (BID) group on Ravelry as our platform of study.  We present our method for measuring bias while taking into account the posting behavior of a user, then apply our method to investigate whether moderators make decisions biased against viewpoints that they may have the incentive to suppress.  We find evidence to suggest that while moderators may make decisions biased against individuals with unpopular viewpoints, the effect of this bias is small and often overblown by the users experiencing bias.We argue that the perception of bias by itself is an issue in online political discussions and suggest technological interventions to counteract the discrepancy between perceived and actual censorship in moderation.},
    author = {Shen, Qinlan and Yoder, Michael Miller and Jo, Yohan and Ros{\'{e}}, Carolyn P.},
    booktitle = {12th International AAAI Conference on Web and Social Media, ICWSM 2018},
    isbn = {9781577357988},
    pages = {350--359},
    title = {{Perceptions of censorship and moderation bias in political debate forums}},
    url = {https://aaai.org/ocs/index.php/ICWSM/ICWSM18/paper/view/17809},
    year = {2018}
}

@inproceedings{huang2015topic,
    address = {Shanghai, China},
    author = {Huang, Xiaolei and Li, Xin and Liu, Tianli and Chiu, David and Zhu, Tingshao and Zhang, Lei},
    booktitle = {Proceedings of the 29th Pacific Asia Conference on Language, Information and Computation},
    title = {{Topic Model for Identifying Suicidal Ideation in Chinese Microblog.}},
    url = {https://www.aclweb.org/anthology/Y15-1064/},
    year = {2015}
}

@article{SHIMODAIRA2000227,
    abstract = {A class of predictive densities is derived by weighting the observed samples in maximizing the log-likelihood function. This approach is eeective in cases such as sample surveys or design of experiments, where the observed covariate follows a diierent distribution than that in the whole population. Under misspeci{\"{y}}cation of the parametric model, the optimal choice of the weight function is asymptotically shown to be the ratio of the density function of the covariate in the population to that in the observations. This is the pseudo-maximum likelihood estima-tion of sample surveys. The optimality is de{\"{y}}ned by the expected Kullback–Leibler loss, and the optimal weight is obtained by considering the importance sampling identity. Under correct speci{\"{y}}cation of the model, however, the ordinary maximum likelihood estimate (i.e. the uniform weight) is shown to be optimal asymptotically. For moderate sample size, the situation is in between the two extreme cases, and the weight function is selected by minimizing a variant of the information criterion derived as an estimate of the expected loss. The method is also applied to a weighted version of the Bayesian predictive density. Numerical examples as well as Monte-Carlo simulations are shown for polynomial regression. A connection with the robust parametric estimation is discussed.},
    author = {Shimodaira, Hidetoshi},
    doi = {https://doi.org/10.1016/S0378-3758(00)00115-4},
    issn = {03783758},
    journal = {Journal of Statistical Planning and Inference},
    keywords = {62B10,62D05,Akaike information criterion,Design of experiments,Importance sampling,Kullback-Leibler divergence,Misspecification,Sample surveys,Weighted least squares},
    number = {2},
    pages = {227--244},
    title = {{Improving predictive inference under covariate shift by weighting the log-likelihood function}},
    url = {http://www.sciencedirect.com/science/article/pii/S0378375800001154},
    volume = {90},
    year = {2000}
}

@inproceedings{kiritchenko2018examining,
    title = "Examining Gender and Race Bias in Two Hundred Sentiment Analysis Systems",
    author = "Kiritchenko, Svetlana  and
      Mohammad, Saif",
    booktitle = "Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/S18-2005",
    doi = "10.18653/v1/S18-2005",
    pages = "43--53",
    abstract = "Automatic machine learning systems can inadvertently accentuate and perpetuate inappropriate human biases. Past work on examining inappropriate biases has largely focused on just individual systems. Further, there is no benchmark dataset for examining inappropriate biases in systems. Here for the first time, we present the Equity Evaluation Corpus (EEC), which consists of 8,640 English sentences carefully chosen to tease out biases towards certain races and genders. We use the dataset to examine 219 automatic sentiment analysis systems that took part in a recent shared task, SemEval-2018 Task 1 {`}Affect in Tweets{'}. We find that several of the systems show statistically significant bias; that is, they consistently provide slightly higher sentiment intensity predictions for one race or one gender. We make the EEC freely available.",
}

@misc{branch_2012,
    author = {Bureau, United States Census},
    booktitle = {Census Bureau QuickFacts},
    month = {sep},
    publisher = {United States Census Bureau},
    title = {{2010 Geographic Terms and Concepts - Census Divisions and Census Regions}},
    url = {https://www.census.gov/geo/reference/gtc/gtc{\_}census{\_}divreg.html},
    year = {2012}
}

@inproceedings{blei2006dynamic,
    abstract = {A family of probabilistic time series models is developed to analyze the time evolution of topics in large document collections. The approach is to use state space models on the natural parameters of the multinomial distributions that represent the topics},
    author = {Blei, David M. and Lafferty, John D.},
    booktitle = {ACM International Conference Proceeding Series},
    doi = {10.1145/1143844.1143859},
    isbn = {1595933832},
    organization = {ACM},
    pages = {113--120},
    title = {{Dynamic topic models}},
    volume = {148},
    year = {2006}
}

@inproceedings{waseem2016you,
    abstract = {Hate speech in the form of racism and sexism is commonplace on the internet (Waseem and Hovy, 2016). For this reason, there has been both an academic and an industry interest in detection of hate speech. The volume of data to be reviewed for creating data sets encour- ages a use of crowd sourcing for the annota- tion efforts. In this paper, we provide an examination of the influence of annotator knowledge of hate speech on classification models by comparing classification results obtained from training on expert and amateur annotations. We provide an evaluation on our own data set and run our models on the data set released by Waseem and Hovy (2016). We},
    title = "Are You a Racist or Am {I} Seeing Things? Annotator Influence on Hate Speech Detection on Twitter",
    author = "Waseem, Zeerak",
    booktitle = "Proceedings of the First Workshop on {NLP} and Computational Social Science",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W16-5618",
    doi = "10.18653/v1/W16-5618",
    pages = "138--142",
}

@inproceedings{bird2004nltk,
    title = "{NLTK}: The Natural Language Toolkit",
    author = "Bird, Steven  and
      Loper, Edward",
    booktitle = "Proceedings of the {ACL} Interactive Poster and Demonstration Sessions",
    month = jul,
    year = "2004",
    address = "Barcelona, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P04-3031",
    pages = "214--217",
}

@inproceedings{hovy2018improving,
    title = "Increasing In-Class Similarity by Retrofitting Embeddings with Demographic Information",
    author = "Hovy, Dirk  and
      Fornaciari, Tommaso",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D18-1070",
    doi = "10.18653/v1/D18-1070",
    pages = "671--677",
    abstract = "Most text-classification approaches represent the input based on textual features, either feature-based or continuous. However, this ignores strong non-linguistic similarities like homophily: people within a demographic group use language more similar to each other than to non-group members. We use homophily cues to retrofit text-based author representations with non-linguistic information, and introduce a trade-off parameter. This approach increases in-class similarity between authors, and improves classification performance by making classes more linearly separable. We evaluate the effect of our method on two author-attribute prediction tasks with various training-set sizes and parameter settings. We find that our method can significantly improve classification performance, especially when the number of labels is large and limited labeled data is available. It is potentially applicable as preprocessing step to any text-classification task.",
}

@inproceedings{Kotsakos:2014:BAD:2600428.2609495,
    abstract = {A large number of mainstream applications, like temporal search, event detection, and trend identification, assume knowledge of the timestamp of every document in a given textual collection. In many cases, however, the required timestamps are either unavailable or ambiguous. A characteristic instance of this problem emerges in the context of large repositories of old digitized documents. For such documents, the timestamp may be corrupted during the digitization process, or may simply be unavailable. In this paper, we study the task of approximating the timestamp of a document, so-called document dating. We propose a content-based method and use recent advances in the domain of term burstiness, which allow it to overcome the drawbacks of pre- vious document dating methods, e.g. the fix time partition strategy. We use an extensive experimental evaluation on different datasets to validate the efficacy and advantages of our methodology, showing that our method outperforms the state of the art methods on document dating.},
    author = {Kotsakos, Dimitrios and Lappas, Theodoros and Kotzias, Dimitrios and Gunopulos, Dimitrios and Kanhabua, Nattiya and N{\o}rv{\aa}g, Kjetil},
    booktitle = {SIGIR 2014 - Proceedings of the 37th International ACM SIGIR Conference on Research and Development in Information Retrieval},
    doi = {10.1145/2600428.2609495},
    isbn = {9781450322591},
    keywords = {Burstiness,Language models,Temporal similarity},
    pages = {1003--1006},
    title = {{A Burstiness-Aware Approach for Document Dating}},
    url = {http://doi.acm.org/10.1145/2600428.2609495},
    year = {2014}
}

@inproceedings{Kanhabua08,
    abstract = {Taking the temporal dimension into account in searching, i.e., using time of content creation as part of the search condition, is now gaining increasingly interest. However, in the case of web search and web warehousing, the timestamps (time of creation or creation of contents) of web pages and documents found on the web are in general not known or can not be trusted, and must be determined otherwise. In this paper, we describe approaches that enhance and increase the quality of existing techniques for determining timestamps based on a temporal language model. Through a number of experiments on temporal document collections we show how our new methods improve the accuracy of timestamping compared to the previous models.},
    author = {Kanhabua, Nattiya and N{\o}rv{\aa}g, Kjetil},
    booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
    doi = {10.1007/978-3-540-87599-4_37},
    isbn = {3540875980},
    issn = {03029743},
    pages = {358--370},
    title = {{Improving temporal language models for determining time of non-timestamped documents}},
    volume = {5173 LNCS},
    year = {2008}
}

@misc{sun2012jieba,
    author = {Sun, J},
    title = {{‘Jieba'Chinese word segmentation tool}},
    urldate = {2018-08-25},
    year = {2012},
    url={https://github.com/fxsjy/jieba}
}

@inproceedings{he2018time,
    abstract = {A unique composite nanonet of metal oxide@carbon interconnected sheets is obtained by atomic layer deposition (ALD)-assisted fabrication. In this nanonet structure, mesoporous metal oxide nanosheets are covered by a layer of amorphous carbon nanoflakes. Specifically, quasi-vertical aligned and mesoporous NixCo1-xO nanosheets are first fabricated directly on nickel foam substrates by a hydrothermal method. Then, an ALD-enabled carbon coating method is applied for the growth of carbon nanoflakes on the surface of the nanosheets. The thus formed 3D hierarchical structure of NixCo1-xO@carbon composite flakes have a higher surface area, better electrical conductivity and structure stability than the bare NixCo1-xO. The application of such composite nanomaterials is demonstrated as electrodes for a supercapacitor and a lithium-ion battery. In both tests, the composite electrode shows enhancement in capacity and cycling stability. This effective composite nanostructure design of metal oxides@carbon flakes could provide a promising method to construct high-performance materials for energy and environment applications. {\textcopyright} 2015 IOP Publishing Ltd.},
    author = {He, Yu and Li, Jianxin and Song, Yangqiu and He, Mutian and Peng, Hao},
    booktitle = {IJCAI International Joint Conference on Artificial Intelligence},
    isbn = {9780999241127},
    issn = {10450823},
    pages = {2241--2247},
    title = {{Time-evolving text classification with deep neural networks}},
    volume = {2018-July},
    year = {2018}
}

@inproceedings{goel2016social,
    author="Goel, Rahul
    and Soni, Sandeep
    and Goyal, Naman
    and Paparrizos, John
    and Wallach, Hanna
    and Diaz, Fernando
    and Eisenstein, Jacob",
    editor="Spiro, Emma
    and Ahn, Yong-Yeol",
    title="The Social Dynamics of Language Change in Online Networks",
    booktitle="Social Informatics",
    year="2016",
    publisher="Springer International Publishing",
    address="Cham",
    pages="41--57",
    abstract="Language change is a complex social phenomenon, revealing pathways of communication and sociocultural influence. But, while language change has long been a topic of study in sociolinguistics, traditional linguistic research methods rely on circumstantial evidence, estimating the direction of change from differences between older and younger speakers. In this paper, we use a data set of several million Twitter users to track language changes in progress. First, we show that language change can be viewed as a form of social influence: we observe complex contagion for phonetic spellings and ``netspeak'' abbreviations (e.g., lol), but not for older dialect markers from spoken language. Next, we test whether specific types of social network connections are more influential than others, using a parametric Hawkes process model. We find that tie strength plays an important role: densely embedded social ties are significantly better conduits of linguistic influence. Geographic locality appears to play a more limited role: we find relatively little evidence to support the hypothesis that individuals are more influenced by geographically local social ties, even in their usage of geographical dialect markers.",
    isbn="978-3-319-47880-7"
}

@book{wilkins1993part,
    author = {Wilkins, D and {at the Max Planck Institute for Psycholinguistics}, Cognitive Anthropology Research Group},
    publisher = {Cognitive Anthropology Research Group at the Max Planck Institute for Psycholinguistics},
    title = {{From part to person: Natural tendencies of semantic change and the search for cognates}},
    url = {https://books.google.com/books?id=YTCqmgEACAAJ},
    year = {1993}
}

@inproceedings{Chambers:2012:LDT:2390524.2390539,
    abstract = {Temporal reasoners for document understand-ing typically assume that a document's cre-ation date is known. Algorithms to ground relative time expressions and order events of-ten rely on this timestamp to assist the learner. Unfortunately, the timestamp is not always known, particularly on the Web. This pa-per addresses the task of automatic document timestamping, presenting two new models that incorporate rich linguistic features about time. The first is a discriminative classifier with new features extracted from the text's time expressions (e.g., 'since 1999'). This model alone improves on previous generative mod-els by 77{\%}. The second model learns prob-abilistic constraints between time expressions and the unknown document time. Imposing these learned constraints on the discriminative model further improves its accuracy. Finally, we present a new experiment design that facil-itates easier comparison by future work.},
    title = "Labeling Documents with Timestamps: Learning from their Time Expressions",
    author = "Chambers, Nathanael",
    booktitle = "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2012",
    address = "Jeju Island, Korea",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P12-1011",
    pages = "98--106",
}

@misc{meituan-dianping_2019,
    author = {Meituan-Dianping},
    booktitle = {Meituan-Dianping},
    month = {feb},
    publisher = {Meituan-Dianping},
    title = {{Meituan-Dianping Official Website}},
    url = {http://www.dianping.com/citylist},
    year = {2019}
}

@misc{miller2012motivational,
    abstract = {Schlagworte: Applications of Motivational Interviewing TS  - Lehmanns Fachbuchhandlung},
    author = {Tober, Gillian},
    booktitle = {Alcohol and Alcoholism},
    doi = {10.1093/alcalc/agt010},
    issn = {0735-0414},
    number = {3},
    pages = {376--377},
    publisher = {New York: The Guilford Press},
    title = {{Motivational Interviewing: Helping People Change}},
    volume = {48},
    year = {2013}
}

@inproceedings{graves2005framewise,
    abstract = {In this paper, we present bidirectional Long Short Term Memory (LSTM) networks, and a modified, full gradient version of the LSTM learning algorithm. We evaluate Bidirectional LSTM (BLSTM) and several other network architectures on the benchmark task of framewise phoneme classification, using the TIMIT database. Our main findings are that bidirectional networks outperform unidirectional ones, and Long Short Term Memory (LSTM) is much faster and also more accurate than both standard Recurrent Neural Nets (RNNs) and time-windowed Multilayer Perceptrons (MLPs). Our results support the view that contextual information is crucial to speech processing, and suggest that BLSTM is an effective architecture with which to exploit it. ?? 2005 Elsevier Ltd. All rights reserved.},
    author = {Graves, Alex and Schmidhuber, J{\"{u}}rgen},
    booktitle = {Proceedings of the International Joint Conference on Neural Networks},
    doi = {10.1109/IJCNN.2005.1556215},
    isbn = {0780390482},
    organization = {IEEE},
    pages = {2047--2052},
    title = {{Framewise phoneme classification with bidirectional LSTM networks}},
    volume = {4},
    year = {2005}
}

@article{yang2017overcoming,
    abstract = {Variation in language is ubiquitous, particularly in newer forms of writing such as social media. Fortunately, variation is not random; it is often linked to social properties of the author. In this paper, we show how to exploit social networks to make sentiment analysis more robust to social language variation. The key idea is linguistic homophily: the tendency of socially linked individuals to use language in similar ways. We formalize this idea in a novel attention-based neural network architecture, in which attention is divided among several basis models, depending on the author's position in the social network. This has the effect of smoothing the classification function across the social network, and makes it possible to induce personalized classifiers even for authors for whom there is no labeled data or demographic metadata. This model significantly improves the accuracies of sentiment analysis on Twitter and on review data.},
    author = {Yang, Yi and Eisenstein, Jacob},
    doi = {10.1162/tacl_a_00062},
    journal = {Transactions of the Association for Computational Linguistics},
    number = {1},
    pages = {295--307},
    title = {{Overcoming Language Variation in Sentiment Analysis with Social Attention}},
    volume = {5},
    year = {2017}
}

@inproceedings{zhang2015character,
    author = {Zhang, Xiang and Zhao, Junbo and LeCun, Yann},
    title = {Character-level Convolutional Networks for Text Classification},
    booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
    series = {NIPS'15},
    year = {2015},
    location = {Montreal, Canada},
    pages = {649--657},
    numpages = {9},
    url = {http://dl.acm.org/citation.cfm?id=2969239.2969312},
    acmid = {2969312},
    publisher = {MIT Press},
    address = {Cambridge, MA, USA},
} 


@inproceedings{zhang2016predicting,
    abstract = {We report an author profling study based on Chinese social media texts gleaned from Sina Weibo in which we attempt to predict the author's age group based on various linguistic text features mainly relating to non-standard orthography: clascal Chinese characters, hashtags, emoticons and kaomoji, homogeneous punctuation and Latin character sequences, and poetic format. We also tracked the use of selected popular Chinese expressions, parts-of-speech and word types. We extracted 100 posts from 100 users in each of four age groups (under-18, 19-29, 30-39, over-40 years) and by clustering users' posts ffty at a time we trained a maximum entropy classifer to predict author age group to an accuracy of 65.5{\%}. We show which features are associated with younger and older age groups, and make our normalisation resources available to other researchers.},
    author = {Zhang, Wanru and Caines, Andrew and Alikaniotis, Dimitrios and Buttery, Paula},
    booktitle = {Proceedings of the 10th International Conference on Language Resources and Evaluation, LREC 2016},
    isbn = {9782951740891},
    keywords = {Computational sociolinguistics,Microblog linguistics,Text forensics,Weibo},
    pages = {2990--2997},
    title = {{Predicting author age from Weibo microblog posts}},
    year = {2016}
}

@inproceedings{heindorf2017wsdm,
    abstract = {{\textcopyright} 2017 ACM. The WSDM Cup 2017 was a data mining challenge held in conjunction with the 10th International Conference onWeb Search and Data Mining (WSDM). It addressed key challenges of knowledge bases today: quality assurance and entity search. For quality assurance, we tackle the task of vandalism detection, based on a dataset of more than 82 million user-contributed revisions of the Wikidata knowledge base, all of which annotated with regard to whether or not they are vandalism. For entity search, we tackle the task of triple scoring, using a dataset that comprises relevance scores for triples from type-like relations including occupation and country of citizenship, based on about 10,000 human relevance judgments. For reproducibility sake, participants were asked to submit their software on TIRA, a cloud-based evaluation platform, and they were incentivized to share their approaches open source.},
    author = {Heindorf, Stefan and Potthast, Martin and Bast, Hannah and Buchhold, Bj{\"{o}}rn and Haussmann, Elmar},
    booktitle = {WSDM 2017 - Proceedings of the 10th ACM International Conference on Web Search and Data Mining},
    doi = {10.1145/3018661.3022762},
    isbn = {9781450346757},
    keywords = {Data quality,Knowledge base,Search,Vandalism},
    pages = {827--828},
    title = {{WSDM Cup 2017: Vandalism detection and triple scoring}},
    url = {https://dl.acm.org/citation.cfm?id=3022762},
    year = {2017}
}

@inproceedings{Burger:2011:DGT:2145432.2145568,
    abstract = {Accurate prediction of demographic attributes from social media and other informal online content is valuable for marketing, personalization, and legal in-vestigation. This paper describes the construction of a large, multilingual dataset labeled with gender, and investigates statistical models for determining the gender of uncharacterized Twitter users. We explore several different classifier types on this dataset. We show the degree to which classifier accuracy varies based on tweet volumes as well as when various kinds of profile metadata are included in the models. We also perform a large-scale human assessment us-ing Amazon Mechanical Turk. Our methods signifi-cantly out-perform both baseline models and almost all humans on the same task.},
    title = "Discriminating Gender on Twitter",
    author = "Burger, John D.  and
      Henderson, John  and
      Kim, George  and
      Zarrella, Guido",
    booktitle = "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing",
    month = jul,
    year = "2011",
    address = "Edinburgh, Scotland, UK.",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D11-1120",
    pages = "1301--1309",
}

@misc{tien_2018,
    author = {{Dana Fontein}},
    booktitle = {Hootsuite Blog},
    month = {oct},
    title = {{Top Facebook Demographics That Matter to Social Media Marketers}},
    url = {https://blog.hootsuite.com/facebook-demographics/},
    year = {2016}
}

@inproceedings{sap2019risk,
    title = "The Risk of Racial Bias in Hate Speech Detection",
    author = "Sap, Maarten  and
      Card, Dallas  and
      Gabriel, Saadia  and
      Choi, Yejin  and
      Smith, Noah A.",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1163",
    doi = "10.18653/v1/P19-1163",
    pages = "1668--1678",
    abstract = "We investigate how annotators{'} insensitivity to differences in dialect can lead to racial bias in automatic hate speech detection models, potentially amplifying harm against minority populations. We first uncover unexpected correlations between surface markers of African American English (AAE) and ratings of toxicity in several widely-used hate speech datasets. Then, we show that models trained on these corpora acquire and propagate these biases, such that AAE tweets and tweets by self-identified African Americans are up to two times more likely to be labelled as offensive compared to others. Finally, we propose *dialect* and *race priming* as ways to reduce the racial bias in annotation, showing that when annotators are made explicitly aware of an AAE tweet{'}s dialect they are significantly less likely to label the tweet as offensive.",
}

@article{blei2003latent,
    author = {Blei, David M and Ng, Andrew Y and Jordan, Michael I},
    journal = {Journal of Machine Learning Research},
    pages = {993--1022},
    title = {{Latent {\{}D{\}}irichlet {\{}A{\}}llocation}},
    url = {http://www.jmlr.org/papers/v3/blei03a.html},
    volume = {3},
    year = {2003}
}

@inproceedings{yang2016hierarchical,
    abstract = {{\textcopyright}2016 Association for Computational Linguistics. We propose a hierarchical attention network for document classification. Our model has two distinctive characteristics: (i) it has a hierarchical structure that mirrors the hierarchical structure of documents; (ii) it has two levels of attention mechanisms applied at the wordand sentence-level, enabling it to attend differentially to more and less important content when constructing the document representation. Experiments conducted on six large scale text classification tasks demonstrate that the proposed architecture outperform previous methods by a substantial margin. Visualization of the attention layers illustrates that the model selects qualitatively informative words and sentences.},
    author = {Yang, Zichao and Yang, Diyi and Dyer, Chris and He, Xiaodong and Smola, Alex and Hovy, Eduard},
    booktitle = {2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT 2016 - Proceedings of the Conference},
    isbn = {9781941643914},
    pages = {1480--1489},
    title = {{Hierarchical attention networks for document classification}},
    year = {2016}
}

@inproceedings{goel2016the,
    abstract = {Language change is a complex social phenomenon, revealing pathways of communication and sociocultural influence. But, while language change has long been a topic of study in sociolinguistics, traditional linguistic research methods rely on circumstantial evidence, estimating the direction of change from differences between older and younger speakers. In this paper, we use a data set of several million Twitter users to track language changes in progress. First, we show that language change can be viewed as a form of social influence: we observe complex contagion for phonetic spellings and "netspeak" abbreviations (e.g., lol), but not for older dialect markers from spoken language. Next, we test whether specific types of social network connections are more influential than others, using a parametric Hawkes process model. We find that tie strength plays an important role: densely embedded social ties are significantly better conduits of linguistic influence. Geographic locality appears to play a more limited role: we find relatively little evidence to support the hypothesis that individuals are more influenced by geographically local social ties, even in their usage of geographical dialect markers.},
    author = {Goel, Rahul and Soni, Sandeep and Goyal, Naman and Paparrizos, John and Wallach, Hanna and Diaz, Fernando and Eisenstein, Jacob},
    booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
    doi = {10.1007/978-3-319-47880-7_3},
    isbn = {9783319478791},
    issn = {16113349},
    month = {nov},
    pages = {41--57},
    title = {{The social dynamics of language change in online networks}},
    url = {https://aclanthology.info/papers/D17-1119/d17-1119 http://aclweb.org/anthology//D/D13/D13-1187.pdf http://aclweb.org/anthology/D/D14/D14-1181.pdf},
    volume = {10046 LNCS},
    year = {2016}
}

@article{tang2018state, 
    title={A state-of-the-art of semantic change computation}, 
    volume={24}, 
    DOI={10.1017/S1351324918000220}, 
    number={5}, 
    journal={Natural Language Engineering}, 
    publisher={Cambridge University Press}, 
    author={TANG, XURI}, 
    year={2018}, 
    pages={649–676}
}

@inproceedings{hu2019diachronic,
    title = "Diachronic Sense Modeling with Deep Contextualized Word Embeddings: An Ecological View",
    author = "Hu, Renfen  and
      Li, Shen  and
      Liang, Shichen",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1379",
    doi = "10.18653/v1/P19-1379",
    pages = "3899--3908",
    abstract = "Diachronic word embeddings have been widely used in detecting temporal changes. However, existing methods face the meaning conflation deficiency by representing a word as a single vector at each time period. To address this issue, this paper proposes a sense representation and tracking framework based on deep contextualized embeddings, aiming at answering not only what and when, but also how the word meaning changes. The experiments show that our framework is effective in representing fine-grained word senses, and it brings a significant improvement in word change detection task. Furthermore, we model the word change from an ecological viewpoint, and sketch two interesting sense behaviors in the process of language evolution, i.e. sense competition and sense cooperation.",
}

@inproceedings{rijhwani2020temporally,
    title = "Temporally-Informed Analysis of Named Entity Recognition",
    author = "Rijhwani, Shruti  and
      Preotiuc-Pietro, Daniel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.680",
    doi = "10.18653/v1/2020.acl-main.680",
    pages = "7605--7617",
    abstract = "Natural language processing models often have to make predictions on text data that evolves over time as a result of changes in language use or the information described in the text. However, evaluation results on existing data sets are seldom reported by taking the timestamp of the document into account. We analyze and propose methods that make better use of temporally-diverse training data, with a focus on the task of named entity recognition. To support these experiments, we introduce a novel data set of English tweets annotated with named entities. We empirically demonstrate the effect of temporal drift on performance, and how the temporal information of documents can be used to obtain better models compared to those that disregard temporal information. Our analysis gives insights into why this information is useful, in the hope of informing potential avenues of improvement for named entity recognition as well as other NLP tasks under similar experimental setups.",
}

@inproceedings{kutuzov2018diachronic,
    title = "Diachronic word embeddings and semantic shifts: a survey",
    author = "Kutuzov, Andrey  and
      {\O}vrelid, Lilja  and
      Szymanski, Terrence  and
      Velldal, Erik",
    booktitle = "Proceedings of the 27th International Conference on Computational Linguistics",
    month = aug,
    year = "2018",
    address = "Santa Fe, New Mexico, USA",
    publisher = "Association for Computational Linguistics",
    pages = "1384--1397",
    abstract = "Recent years have witnessed a surge of publications aimed at tracing temporal changes in lexical semantics using distributional methods, particularly prediction-based word embedding models. However, this vein of research lacks the cohesion, common terminology and shared practices of more established areas of natural language processing. In this paper, we survey the current state of academic research related to diachronic word embeddings and semantic shifts detection. We start with discussing the notion of semantic shifts, and then continue with an overview of the existing methods for tracing such time-related shifts with word embedding models. We propose several axes along which these methods can be compared, and outline the main challenges before this emerging subfield of NLP, as well as prospects and possible applications.",
}

@inproceedings{heindorf2019debiasing,
    author = {Heindorf, Stefan and Scholten, Yan and Engels, Gregor and Potthast, Martin},
    title = {Debiasing Vandalism Detection Models at Wikidata},
    booktitle = {The World Wide Web Conference},
    series = {WWW '19},
    year = {2019},
    isbn = {978-1-4503-6674-8},
    location = {San Francisco, CA, USA},
    pages = {670--680},
    numpages = {11},
    url = {http://doi.acm.org/10.1145/3308558.3313507},
    doi = {10.1145/3308558.3313507},
    acmid = {3313507},
    publisher = {ACM},
    address = {New York, NY, USA},
}

@inproceedings{Wijaya:2011:USC:2064448.2064475,
    abstract = {In this paper, we propose to model and analyze changes that occur to an entity in terms of changes in the words that co-occur with the entity over time. We propose to do an in-depth analysis of how this co-occurrence changes over time, how the change influences the state (semantic, role) of the entity, and how the change may correspond to events occurring in the same period of time. We propose to identify clusters of topics surrounding the entity over time using Topics-Over-Time (TOT) and k-means clustering. We conduct this analysis on Google Books Ngram dataset. We show how clustering words that co-occur with an entity of interest in 5-grams can shed some lights to the nature of change that occurs to the entity and identify the period for which the change occurs. We find that the period identified by our model precisely coincides with events in the same period that correspond to the change that occurs.},
    author = {Wijaya, Derry Tanti and Yeniterzi, Reyyan},
    booktitle = {International Conference on Information and Knowledge Management, Proceedings},
    doi = {10.1145/2064448.2064475},
    isbn = {9781450309622},
    keywords = {event detection,semantic change,topic clustering,topic transition over time},
    pages = {35--40},
    title = {{Understanding semantic change of words over centuries}},
    url = {http://doi.acm.org/10.1145/2064448.2064475},
    year = {2011}
}

@inproceedings{xiao2016behavioral,
    abstract = {Copyright {\textcopyright}2016 ISCA.Manual annotation of human behaviors with domain specific codes is a primary method of research and treatment fidelity evaluation in psychotherapy. However, manual annotation has a prohibitively high cost and does not scale to coding large amounts of psychotherapy session data. In this paper, we present a case study of modeling therapist language in addiction counseling, and propose an automatic coding approach. The task objective is to code therapist utterances with domain specific codes. We employ Recurrent Neural Networks (RNNs) to predict these behavioral codes based on session transcripts. Experiments show that RNNs outperform the baseline method using Maximum Entropy models. The model with bi-directional Gated Recurrent Units and domain specific word embeddings achieved the highest overall accuracy. We also briefly discuss about client code prediction and comparison to previous work.},
    author = {Xiao, Bo and Can, Doǧan and Gibson, James and Imel, Zac E. and Atkins, David C. and Georgiou, Panayiotis and Narayanan, Shrikanth},
    booktitle = {Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
    issn = {19909772},
    keywords = {Behavioral coding,Language modeling,Motivational interviewing,Recurrent neural network},
    pages = {908--912},
    title = {{Behavioral coding of therapist language in addiction counseling using recurrent neural networks}},
    volume = {08-12-Sept},
    year = {2016}
}

@misc{nsduh2015,
    author = {NIAAA},
    booktitle = {National Institute on Alcohol Abuse and Alcoholism},
    doi = {10.1016/j.adolescence.2017.10.004},
    isbn = {1533-4406 (Electronic)$\backslash$r0028-4793 (Linking)},
    issn = {10959254},
    keywords = {Alcohol use,alcohol use d,alcohol-related deaths},
    month = {feb},
    pmid = {29065357},
    publisher = {U.S. Department of Health and Human Services},
    title = {{Alcohol Facts and Statistics}},
    url = {https://www.niaaa.nih.gov/alcohol-facts-and-statistics},
    year = {2017}
}

@article{carey2009computer,
    abstract = {In this study, the authors evaluated the efficacy of a brief motivational intervention (BMI) and a computerized program for reducing drinking and related problems among college students sanctioned for alcohol violations. Referred students (N = 198, 46{\%} women), stratified by gender, were randomly assigned to a BMI or to the Alcohol 101 Plus computer program. Data obtained at baseline, 1, 6, and 12 months were used to evaluate intervention efficacy. Planned analyses revealed 3 primary findings. First, women who received the BMI reduced drinking more than did women who received the computer intervention; in contrast, men's drinking reductions did not differ by condition. Second, readiness to change and hazardous drinking status predicted drinking reductions at 1 month postintervention, regardless of intervention. Third, by 1 year, drinking returned to presanction (baseline) levels, with no differences in recidivism between groups. Exploratory analyses revealed an overall mean reduction in drinking immediately after the sanction event and before taking part in an intervention. Furthermore, after the self-initiated reductions prompted by the sanction were accounted for, participation in the BMI but not the computer intervention was found to produce additional reduction in drinking and related consequences. {\textcopyright} 2009 American Psychological Association.},
    author = {Carey, Kate B. and Henson, James M. and Carey, Michael P. and Maisto, Stephen A.},
    doi = {10.1037/a0014281},
    issn = {0022006X},
    journal = {Journal of Consulting and Clinical Psychology},
    keywords = {alcohol abuse prevention,brief intervention,college drinking,gender,mandated students},
    number = {1},
    pages = {74--87},
    publisher = {American Psychological Association},
    title = {{Computer Versus In-Person Intervention for Students Violating Campus Alcohol Policy}},
    volume = {77},
    year = {2009}
}

@inproceedings{EisensteinEtAl10,
    title = "A Latent Variable Model for Geographic Lexical Variation",
    author = "Eisenstein, Jacob  and
      O{'}Connor, Brendan  and
      Smith, Noah A.  and
      Xing, Eric P.",
    booktitle = "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing",
    month = oct,
    year = "2010",
    address = "Cambridge, MA",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D10-1124",
    pages = "1277--1287",
}

@inproceedings{Rao:2010:CLU:1871985.1871993,
    abstract = {Social media outlets such as Twitter have become an impor- tant forum for peer interaction. Thus the ability to classify latent user attributes, including gender, age, regional origin, and political orientation solely fromTwitter user language or similar highly informal content has important applications in advertising, personalization, and recommendation. This paper includes a novel investigation of stacked-SVM-based classification algorithms over a rich set of original features, applied to classifying these four user attributes. It also in- cludes extensive analysis of features and approaches that are effective and not effective in classifying user attributes in Twitter-style informal written genres as distinct from the other primarily spoken genres previously studied in the user- property classification literature. Our models, singly and in ensemble, significantly outperform baseline models in all cases. A detailed analysis of model components and fea- tures provides an often entertaining insight into distinctive language-usage variation across gender, age, regional origin and political orientation inmodern informal communication.},
    author = {Rao, Delip and Yarowsky, David and Shreevats, Abhishek and Gupta, Manaswi},
    title = {Classifying Latent User Attributes in Twitter},
    booktitle = {Proceedings of the 2Nd International Workshop on Search and Mining User-generated Contents},
    series = {SMUC '10},
    year = {2010},
    isbn = {978-1-4503-0386-6},
    location = {Toronto, ON, Canada},
    pages = {37--44},
    numpages = {8},
    url = {http://doi.acm.org/10.1145/1871985.1871993},
    doi = {10.1145/1871985.1871993},
    acmid = {1871993},
    publisher = {ACM},
    address = {New York, NY, USA},
    keywords = {attribute learning, latent attribute classification, social media},
}

@article{bojanowski2017enriching,
    abstract = {Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character n-grams. A vector representation is associated to each character n-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.},
    author = {Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
    doi = {10.1162/tacl_a_00051},
    journal = {Transactions of the Association for Computational Linguistics},
    pages = {135--146},
    title = {{Enriching Word Vectors with Subword Information}},
    volume = {5},
    year = {2017}
}

@inproceedings{wing2014hierarchical,
    title = "Hierarchical Discriminative Classification for Text-Based Geolocation",
    author = "Wing, Benjamin  and
      Baldridge, Jason",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D14-1039",
    doi = "10.3115/v1/D14-1039",
    pages = "336--348",
}

@article{arkowitz2004integrating,
    abstract = {Many clients engaging in Cognitive Behavioral Therapy (CBT) for depression and anxiety are ambivalent about change, and about taking necessary actions to bring about change such as exposure or behavioral activation exercises. Given the focus of motivational interviewing (MI) on enhancing readiness for change, it is of great interest to investigate applications of MI to prevalent disorders such as depression and anxiety. After exploring the rationale for integrating MI with CBT for these disorders, we outline unique features of MI that may render it a useful complement to CBT, such as its focus on resolving ambivalence for change and specific strategies for responding to resistance. We suggest several possible ways in which MI may be combined with CBT. Finally, we discuss our clinical experience with adapting MI to the treatment of depression and anxiety, including case illustrations of each, and discuss some of the unique issues arising in generalizing MI for use with these populations.},
    author = {Arkowitz, Hal and Westra, Henny A.},
    doi = {10.1891/jcop.18.4.337.63998},
    issn = {08898391},
    journal = {Journal of Cognitive Psychotherapy},
    number = {4},
    pages = {337--350},
    publisher = {Springer Publishing Company},
    title = {{Integrating motivational interviewing and cognitive behavioral therapy in the treatment of depression and anxiety}},
    volume = {18},
    year = {2004}
}

@inproceedings{blodgett2016demographic,
    title = "Demographic Dialectal Variation in Social Media: A Case Study of {A}frican-{A}merican {E}nglish",
    author = "Blodgett, Su Lin  and
      Green, Lisa  and
      O{'}Connor, Brendan",
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D16-1120",
    doi = "10.18653/v1/D16-1120",
    pages = "1119--1130",
}

@inproceedings{zhang2017temporal,
    author = {Zhang, Yating and Jatowt, Adam and Tanaka, Katsumi},
    title = {Temporal Analog Retrieval Using Transformation over Dual Hierarchical Structures},
    booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
    series = {CIKM '17},
    year = {2017},
    isbn = {978-1-4503-4918-5},
    location = {Singapore, Singapore},
    pages = {717--726},
    numpages = {10},
    url = {http://doi.acm.org/10.1145/3132847.3132917},
    doi = {10.1145/3132847.3132917},
    acmid = {3132917},
    publisher = {ACM},
    address = {New York, NY, USA},
    keywords = {cluster-biased, dual hierarchical structure, heterogeneous document collections, temporal analog},
}

@inproceedings{rehurek2010software,
    author = {Rehurek, Radim and Sojka, Petr},
    booktitle = {Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks},
    isbn = {2-9517408-6-7},
    issn = {2951740867},
    month = {may},
    pages = {45--50},
    publisher = {ELRA},
    title = {{Software Framework for Topic Modelling with Large Corpora}},
    url = {https://radimrehurek.com/gensim/lrec2010{\_}final.pdf},
    year = {2010}
}

@book{coulmas_2017,
    abstract = {This accessible new textbook provides a clear introduction to sociolinguistics, the study of why we speak the way we do, and the social factors influencing our linguistic descisions. Based on the notion of 'choice'--that as speakers we select from the options open to us--it provides a solid theoretical framework to deal with variability and diversity in language. Sociolinguistics: The Study of Speakers' Choices will become a key text for all students of sociolinguistics, and will be welcomed by anyone interested in the complex interaction between language and society.},
    author = {Coulmas, Florian},
    booktitle = {Sociolinguistics: The Study of Speakers' Choices, Second Edition},
    doi = {10.1017/CBO9781139794732},
    isbn = {9781139794732},
    pages = {1--307},
    publisher = {Cambridge University Press},
    title = {{Sociolinguistics: The study of speakers' choices, second edition}},
    url = {https://www.cambridge.org/us/academic/subjects/languages-linguistics/sociolinguistics/sociolinguistics-study-speakers-choices-2nd-edition?format=HB{\&}isbn=9781107037649},
    year = {2010}
}

@misc{chollet2015keras,
    author = {Chollet, Fran{\c{c}}ois and Others},
    publisher = {GitHub},
    title = {{Keras}},
    url = {https://keras.io},
    urldate = {08-19-2017},
    year = {2015}
}

@inproceedings{huang2019neuraluser,
    abstract = {Language use varies across different demographic factors, such as gender, age, and geographic location. However, most existing document classification methods ignore demographic variability. In this study, we examine empirically how text data can vary across four demographic factors: gender, age, country, and region. We propose a multitask neural model to account for demographic variations via adversarial training. In experiments on four English-language social media datasets, we find that classification performance improves when adapting for user factors.},
    address = {Minneapolis, Minnesota},
    author = {Huang, Xiaolei and Paul, Michael J.},
    booktitle = {Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*{\{}SEM{\}} 2019)},
    doi = {10.18653/v1/S19-1015},
    isbn = {9781948087346},
    issn = {1532-4435},
    month = {jun},
    organization = {Association for Computational Linguistics},
    pages = {136--146},
    publisher = {Association for Computational Linguistics},
    title = {{Neural User Factor Adaptation for Text Classification: Learning to Generalize Across Author Demographics}},
    url = {https://www.aclweb.org/anthology/S19-1015},
    volume = {4},
    year = {2019}
}

@inproceedings{daume2007frustratingly,
    abstract = {We describe an approach to domain adaptation that is appropriate exactly in the case when one has enough ``target'' data to do slightly better than just using only ``source'' data. Our approach is incredibly simple, easy to implement as a preprocessing step (10 lines of Perl!) and outperforms state-of-the-art approaches on a range of datasets. Moreover, it is trivially extended to a multi-domain adaptation problem, where one has data from a variety of different domains.},
    address = {Prague, Czech Republic},
    author = {Daume, Hal},
    booktitle = {ACL 2007 - Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics},
    isbn = {9781932432862},
    pages = {256--263},
    publisher = {Association for Computational Linguistics},
    title = {{Frustratingly Easy Domain Adaptation}},
    url = {https://www.aclweb.org/anthology/P07-1033},
    year = {2007}
}

@article{huang2019can,
    author = {Huang, Xiaolei and Smith, Michael C and Jamison, Amelia M and Broniatowski, David A and Dredze, Mark and Quinn, Sandra Crouse and Cai, Justin and Paul, Michael J},
    doi = {10.1136/bmjopen-2018-024018},
    issn = {2044-6055},
    journal = {BMJ Open},
    month = {jan},
    number = {1},
    pages = {e024018},
    publisher = {British Medical Journal Publishing Group},
    title = {{Can online self-reports assist in real-time identification of influenza vaccination uptake? A cross-sectional study of influenza vaccine-related tweets in the USA, 2013–2017}},
    url = {http://bmjopen.bmj.com/lookup/doi/10.1136/bmjopen-2018-024018},
    volume = {9},
    year = {2019}
}

@inproceedings{huang2019neural,
    address = {Florence, Italy},
    author = {Huang, Xiaolei and Paul, Michael J},
    booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
    doi = {10.18653/v1/P19-1403},
    month = {jul},
    pages = {4113--4123},
    publisher = {Association for Computational Linguistics},
    title = {{Neural Temporality Adaptation for Document Classification: Diachronic Word Embeddings and Domain Adaptation Models}},
    url = {https://www.aclweb.org/anthology/P19-1403},
    year = {2019}
}

@inproceedings{huang2017exploring,
    author = {Huang, Xiaolei and Xing, Linzi and Brubaker, Jed R and Paul, Michael J},
    booktitle = {2017 IEEE International Conference on Healthcare Informatics (ICHI)},
    organization = {IEEE},
    pages = {470--477},
    title = {{Exploring timelines of confirmed suicide incidents through social media}},
    url = {https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8031196},
    year = {2017}
}

@inproceedings{lamb2013separating,
    title = "Separating Fact from Fear: Tracking Flu Infections on Twitter",
    author = "Lamb, Alex  and
      Paul, Michael J.  and
      Dredze, Mark",
    booktitle = "Proceedings of the 2013 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2013",
    address = "Atlanta, Georgia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N13-1097",
    pages = "789--795",
}

@inproceedings{zhu2019detecting,
  title={Detecting depression from Internet behaviors by time-frequency features},
  author={Zhu, Changye and Li, Baobin and Li, Ang and Zhu, Tingshao},
  booktitle={Web Intelligence},
  volume={17},
  pages={199--208},
  year={2019},
  organization={IOS Press}
}

@inproceedings{de2016discovering,
  title={Discovering shifts to suicidal ideation from mental health content in social media},
  author={De Choudhury, Munmun and Kiciman, Emre and Dredze, Mark and Coppersmith, Glen and Kumar, Mrinal},
  booktitle={Proceedings of the 2016 CHI conference on human factors in computing systems},
  pages={2098--2110},
  year={2016},
  organization={ACM}
}

@inproceedings{zheng2014context,
 author = {Zheng, Yong and Mobasher, Bamshad and Burke, Robin},
 title = {Context Recommendation Using Multi-label Classification},
 booktitle = {Proceedings of the 2014 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT) - Volume 02},
 series = {WI-IAT '14},
 year = {2014},
 isbn = {978-1-4799-4143-8},
 pages = {288--295},
 numpages = {8},
 url = {http://dx.doi.org/10.1109/WI-IAT.2014.110},
 doi = {10.1109/WI-IAT.2014.110},
 acmid = {2682858},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
 keywords = {recommender systems, context recommendation, context, context-aware, context-aware recommender systems},
}

@inproceedings{felbo2017using,
    title = "Using millions of emoji occurrences to learn any-domain representations for detecting sentiment, emotion and sarcasm",
    author = "Felbo, Bjarke  and
      Mislove, Alan  and
      S{\o}gaard, Anders  and
      Rahwan, Iyad  and
      Lehmann, Sune",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D17-1169",
    doi = "10.18653/v1/D17-1169",
    pages = "1615--1625",
    abstract = "NLP tasks are often limited by scarcity of manually annotated data. In social media sentiment analysis and related tasks, researchers have therefore used binarized emoticons and specific hashtags as forms of distant supervision. Our paper shows that by extending the distant supervision to a more diverse set of noisy labels, the models can learn richer representations. Through emoji prediction on a dataset of 1246 million tweets containing one of 64 common emojis we obtain state-of-the-art performance on 8 benchmark datasets within emotion, sentiment and sarcasm detection using a single pretrained model. Our analyses confirm that the diversity of our emotional labels yield a performance improvement over previous distant supervision approaches.",
}

@inproceedings{gayo2011limits,
	author = {Daniel Gayo-Avello and Panagiotis Metaxas and Eni Mustafaraj},
	title = {Limits of Electoral Predictions Using Twitter},
	booktitle = {International AAAI Conference on Web and Social Media},
	year = {2011},
	keywords = {},
	abstract = {Using social media for political discourse is becoming common practice, especially around election time. One interesting aspect of this trend is the possibility of pulsing the public’s opinion about the elections, and that has attracted the interest of many researchers and the press. Allegedly, predicting electoral outcomes from social media data can be feasible and even simple. Positive results have been reported, but without an analysis on what principle enables them. Our work puts to test the purported predictive power of socialmedia metrics against the 2010 US congressional elections. Here, we applied techniques that had reportedly led to positive election predictions in the past, on the Twitter data collected from the 2010 US congressional elections. Unfortunately, we find no correlation between the analysis results and the electoral outcomes, contradicting previous reports. Observing that 80 years of polling research would support our findings, we argue that one should not be accepting predictions about events using social media data as a black box. Instead, scholarly research should be accompanied by a model explaining the predictive power of social media, when there is one.},
	url = {https://www.aaai.org/ocs/index.php/ICWSM/ICWSM11/paper/view/2862/3254}
}

@book{goodfellow2016deep,
    author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
    title = {Deep Learning},
    year = {2016},
    isbn = {0262035618, 9780262035613},
    publisher = {The MIT Press},
} 

@inproceedings{pascanu2013difficulty,
    title={On the difficulty of training recurrent neural networks},
    author={Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
    booktitle={International Conference on Machine Learning},
    pages={1310--1318},
    year={2013},
    url={http://proceedings.mlr.press/v28/pascanu13.pdf}
}

@article{gayo2013predicting,
  title={Predicting information credibility in time-sensitive social media},
  author={Gayo-Avello, Daniel and Metaxas, Panagiotis Takis and Mustafaraj, Eni and Strohmaier, Markus and Schoen, Harald and Gloor, Peter and Castillo, Carlos and Mendoza, Marcelo and Poblete, Barbara},
  journal={Internet Research},
  volume = {23},
  number={5},
  year={2013},
  pages = "560-588",
  publisher={Emerald Group Publishing Limited},
  doi={10.1108/IntR-05-2012-0095},
  url={https://doi.org/10.1108/IntR-05-2012-0095}
}

@inproceedings{lim2019smart,
    author = {Lim, Hyoyoung and Huang, Xiaolei and Miller, Samuel and Edelmann, Joshua and Euken, Timothy and Voida, Stephen},
    title = {Smart Cook: Making Cooking Easier with Multimodal Learning},
    booktitle = {Adjunct Proceedings of the 2019 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2019 ACM International Symposium on Wearable Computers},
    series = {UbiComp/ISWC '19 Adjunct},
    year = {2019},
    isbn = {978-1-4503-6869-8},
    location = {London, United Kingdom},
    pages = {129--132},
    numpages = {4},
    url = {http://doi.acm.org/10.1145/3341162.3343836},
    doi = {10.1145/3341162.3343836},
    acmid = {3343836},
    publisher = {ACM},
    address = {New York, NY, USA},
    keywords = {deep learning, domestic computing, recipe recommendation system, ubiquitous computing design},
} 

@inproceedings{zhao2019gender,
    title = "Gender Bias in Contextualized Word Embeddings",
    author = "Zhao, Jieyu  and
      Wang, Tianlu  and
      Yatskar, Mark  and
      Cotterell, Ryan  and
      Ordonez, Vicente  and
      Chang, Kai-Wei",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1064",
    doi = "10.18653/v1/N19-1064",
    pages = "629--634",
    abstract = "In this paper, we quantify, analyze and mitigate gender bias exhibited in ELMo{'}s contextualized word vectors. First, we conduct several intrinsic analyses and find that (1) training data for ELMo contains significantly more male than female entities, (2) the trained ELMo embeddings systematically encode gender information and (3) ELMo unequally encodes gender information about male and female entities. Then, we show that a state-of-the-art coreference system that depends on ELMo inherits its bias and demonstrates significant bias on the WinoBias probing corpus. Finally, we explore two methods to mitigate such gender bias and show that the bias demonstrated on WinoBias can be eliminated.",
}

@inproceedings{zhao2018learning,
    title = "Learning Gender-Neutral Word Embeddings",
    author = "Zhao, Jieyu  and
      Zhou, Yichao  and
      Li, Zeyu  and
      Wang, Wei  and
      Chang, Kai-Wei",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D18-1521",
    doi = "10.18653/v1/D18-1521",
    pages = "4847--4853",
    abstract = "Word embedding models have become a fundamental component in a wide range of Natural Language Processing (NLP) applications. However, embeddings trained on human-generated corpora have been demonstrated to inherit strong gender stereotypes that reflect social constructs. To address this concern, in this paper, we propose a novel training procedure for learning gender-neutral word embeddings. Our approach aims to preserve gender information in certain dimensions of word vectors while compelling other dimensions to be free of gender influence. Based on the proposed method, we generate a Gender-Neutral variant of GloVe (GN-GloVe). Quantitative and qualitative experiments demonstrate that GN-GloVe successfully isolates gender information without sacrificing the functionality of the embedding model.",
}

@inproceedings{wu2018starspace,
	author = {Ledell Wu and Adam Fisch and Sumit Chopra and Keith Adams and Antoine Bordes and Jason Weston},
	title = {StarSpace: Embed All The Things!},
	booktitle = {Thirty-Second AAAI Conference on Artificial Intelligence},
	address={New Orleans, USA},
	year = {2018},
	keywords = {Nature Language Processing; Text Classification; Knowledge Representation; Recommender Systems},
	abstract = {We present StarSpace, a general-purpose neural embedding model that can solve a wide variety of problems: labeling tasks such as text classification,ranking tasks such as information retrieval/web search,collaborative filtering-based  or content-based recommendation,embedding of multi-relational graphs, and learning word, sentence or document level embeddings.In each case the model works by embedding those entities comprised of discrete features and comparing them against each other -- learning similarities dependent on the task.Empirical results on a number of tasks show that StarSpace is highly competitive with existing methods, whilst also being generally applicable to new cases where those methods are not.},
	url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16998/16114}
}

@inproceedings{devlin2019bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}

@article{yang2019xlnet,
  title={XLNet: Generalized Autoregressive Pretraining for Language Understanding},
  author={Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Ruslan and Le, Quoc V},
  journal={arXiv preprint arXiv:1906.08237},
  year={2019},
  url={https://arxiv.org/pdf/1906.08237.pdf}
}


% user embeddings
@inproceedings{pan2019social,
  title     = {Social Media-based User Embedding: A Literature Review},
  author    = {Pan, Shimei and Ding, Tao},
  booktitle = {Proceedings of the Twenty-Eighth International Joint Conference on
               Artificial Intelligence, {IJCAI-19}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},             
  pages     = {6318--6324},
  year      = {2019},
  month     = {7},
  doi       = {10.24963/ijcai.2019/881},
  url       = {https://doi.org/10.24963/ijcai.2019/881},
}

@inproceedings{benton2016learning,
    title = "Learning Multiview Embeddings of Twitter Users",
    author = "Benton, Adrian  and
      Arora, Raman  and
      Dredze, Mark",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P16-2003",
    doi = "10.18653/v1/P16-2003",
    pages = "14--19",
}

@inproceedings{liang2018dynamic,
 author = {Liang, Shangsong and Zhang, Xiangliang and Ren, Zhaochun and Kanoulas, Evangelos},
 title = {Dynamic Embeddings for User Profiling in Twitter},
 booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery \&\#38; Data Mining},
 series = {KDD '18},
 year = {2018},
 isbn = {978-1-4503-5552-0},
 location = {London, United Kingdom},
 pages = {1764--1773},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/3219819.3220043},
 doi = {10.1145/3219819.3220043},
 acmid = {3220043},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dynamic model, profiling, word embeddings},
} 

@inproceedings{kumar2019predicting,
 author = {Kumar, Srijan and Zhang, Xikun and Leskovec, Jure},
 title = {Predicting Dynamic Embedding Trajectory in Temporal Interaction Networks},
 booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
 series = {KDD '19},
 year = {2019},
 isbn = {978-1-4503-6201-6},
 location = {Anchorage, AK, USA},
 pages = {1269--1278},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/3292500.3330895},
 doi = {10.1145/3292500.3330895},
 acmid = {3330895},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {deep learning, embeddings},
} 

@inproceedings{ding2017multi,
    title = "Multi-View Unsupervised User Feature Embedding for Social Media-based Substance Use Prediction",
    author = "Ding, Tao  and
      Bickel, Warren K.  and
      Pan, Shimei",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D17-1241",
    doi = "10.18653/v1/D17-1241",
    pages = "2275--2284",
    abstract = "In this paper, we demonstrate how the state-of-the-art machine learning and text mining techniques can be used to build effective social media-based substance use detection systems. Since a substance use ground truth is difficult to obtain on a large scale, to maximize system performance, we explore different unsupervised feature learning methods to take advantage of a large amount of unsupervised social media data. We also demonstrate the benefit of using multi-view unsupervised feature learning to combine heterogeneous user information such as Facebook {``}likes{''} and {``}status updates{''} to enhance system performance. Based on our evaluation, our best models achieved 86{\%} AUC for predicting tobacco use, 81{\%} for alcohol use and 84{\%} for illicit drug use, all of which significantly outperformed existing methods. Our investigation has also uncovered interesting relations between a user{'}s social media behavior (e.g., word usage) and substance use.",
}

@inproceedings{ding2018predicting,
    author={T. {Ding} and W. K. {Bickel} and S. {Pan}},
    booktitle={2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)},
    title={Predicting Delay Discounting from Social Media Likes with Unsupervised Feature Learning},
    year={2018},
    volume={},
    number={},
    pages={254-257},
    keywords={behavioural sciences;decision making;social networking (online);unsupervised learning;unsupervised feature learning;human decision making;behavior science;DDR;real-world behavior;large-scale social media analytics;feature space;social media likes;behavioral measure;delay discounting rate;social media behavior;Delays;Facebook;Learning systems;Context modeling;Economics;Task analysis},
    doi={10.1109/ASONAM.2018.8508277},
    ISSN={},
    month={Aug},
}

@inproceedings{amir2017quantifying,
  title = 	 {Quantifying Mental Health from Social Media with Neural User Embeddings},
  author = 	 {Silvio Amir and Glen Coppersmith and Paula Carvalho and Mario J. Silva and Bryon C. Wallace},
  booktitle = 	 {Proceedings of the 2nd Machine Learning for Healthcare Conference},
  pages = 	 {306--321},
  year = 	 {2017},
  editor = 	 {Finale Doshi-Velez and Jim Fackler and David Kale and Rajesh Ranganath and Byron Wallace and Jenna Wiens},
  volume = 	 {68},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Boston, Massachusetts},
  month = 	 {18--19 Aug},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v68/amir17a/amir17a.pdf},
  url = 	 {http://proceedings.mlr.press/v68/amir17a.html},
  abstract = 	 {Mental illnesses adversely affect a significant proportion of the population worldwide. However, the typical methods to estimate and characterize the prevalence of mental health conditions are time-consuming and expensive. Consequently, best-available estimates concerning the prevalence of these conditions are often years out of date. Automated approaches that supplement traditional methods with broad, aggregated information derived from social media provide a potential means of furnishing near real-time estimates at scale. These may in turn provide grist for supporting, evaluating and iteratively improving public health programs and interventions. We propose a novel approach for mental health quantification that leverages user em-beddings induced from social media post histories. Recent work showed that learned user representations capture latent aspects of individuals (e.g., political leanings). This paper investigates whether these representations also correlate with mental health statuses. To this end, we induced embeddings for a set of users known to be affected by depression and post-traumatic stress disorder, and for a set of demographically matched ‘control’ users. We then evaluated the induced user representations with respect to: (i) their ability to capture homophilic relations with respect to mental health statuses; and (ii) their predictive performance in downstream mental health models. Our experimental results demonstrate that learned user embeddings capture relevant signals for mental health quantification.}
}

@inproceedings{amir2016modelling,
    title = "Modelling Context with User Embeddings for Sarcasm Detection in Social Media",
    author = "Amir, Silvio  and
      Wallace, Byron C.  and
      Lyu, Hao  and
      Carvalho, Paula  and
      Silva, M{\'a}rio J.",
    booktitle = "Proceedings of The 20th {SIGNLL} Conference on Computational Natural Language Learning",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/K16-1017",
    doi = "10.18653/v1/K16-1017",
    pages = "167--177",
}

@inproceedings{zhang2015using,
    author="Zhang, Lei
    and Huang, Xiaolei
    and Liu, Tianli
    and Li, Ang
    and Chen, Zhenxiang
    and Zhu, Tingshao",
    editor="Zu, Qiaohong
    and Hu, Bo
    and Gu, Ning
    and Seng, Sopheap",
    title="Using Linguistic Features to Estimate Suicide Probability of Chinese Microblog Users",
    booktitle="Human Centered Computing",
    year="2015",
    publisher="Springer International Publishing",
    address="Cham",
    pages="549--559",
    abstract="If people with high risk of suicide can be identified through social media like microblog, it is possible to implement an active intervention system to save their lives. Based on this motivation, the current study administered the Suicide Probability Scale(SPS) to 1041 weibo users at Sina Weibo, which is a leading microblog service provider in China. Two NLP (Natural Language Processing) methods, the Chinese edition of Linguistic Inquiry and Word Count (LIWC) lexicon and Latent Dirichlet Allocation (LDA), are used to extract linguistic features from the Sina Weibo data. We trained predicting models by machine learning algorithm based on these two types of features, to estimate suicide probability based on linguistic features. The experiment results indicate that LDA can find topics that relate to suicide probability, and improve the performance of prediction. Our study adds value in prediction of suicidal probability of social network users with their behaviors.",
    isbn="978-3-319-15554-8"
}

@inproceedings{pennacchiotti2011a,
	author = {Marco Pennacchiotti and Ana-Maria Popescu},
	title = {A Machine Learning Approach to Twitter User Classification},
	booktitle = {International AAAI Conference on Web and Social Media},
	year = {2011},
	keywords = {},
	abstract = {This paper addresses the task of user classification in social media, with an application to Twitter. We automatically infer the values of user attributes such as political orientation or ethnicity  by leveraging observable information such as the user behavior, network structure and the linguistic content of the user’s Twitter feed. We employ a machine learning approach which relies on a comprehensive set of features derived from such user information. We report encouraging experimental results on 3 tasks with different characteristics: political affiliation detection, ethnicity identification and detecting affinity for a particular business. Finally, our analysis shows that rich linguistic features  prove consistently valuable across the 3 tasks and show great promise for additional user classification needs.},
	url = {https://www.aaai.org/ocs/index.php/ICWSM/ICWSM11/paper/view/2886/3262}
}

@inproceedings{huang2020multilingual,
    title = "Multilingual {T}witter Corpus and Baselines for Evaluating Demographic Bias in Hate Speech Recognition",
    author = "Huang, Xiaolei  and
      Xing, Linzi  and
      Dernoncourt, Franck  and
      Paul, Michael J.",
    booktitle = "Proceedings of The 12th Language Resources and Evaluation Conference",
    month = may,
    year = "2020",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://www.aclweb.org/anthology/2020.lrec-1.180",
    pages = "1440--1448",
    language = "English",
    ISBN = "979-10-95546-34-4",
}

@inproceedings{huang2019matters,
  title={What Matters for Neural Cross-Lingual Named Entity Recognition: An Empirical Analysis},
  author={Huang, Xiaolei and May, Jonathan and Peng, Nanyun},
  booktitle={2019 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year={2019},
  address={Hong Kong, China},
  url={https://arxiv.org/abs/1909.03598},
}

% multilingual fairness
@inproceedings{basile2019semeval,
    title = "{S}em{E}val-2019 Task 5: Multilingual Detection of Hate Speech Against Immigrants and Women in Twitter",
    author = "Basile, Valerio  and
      Bosco, Cristina  and
      Fersini, Elisabetta  and
      Nozza, Debora  and
      Patti, Viviana  and
      Rangel Pardo, Francisco Manuel  and
      Rosso, Paolo  and
      Sanguinetti, Manuela",
    booktitle = "Proceedings of the 13th International Workshop on Semantic Evaluation",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/S19-2007",
    doi = "10.18653/v1/S19-2007",
    pages = "54--63",
    abstract = "The paper describes the organization of the SemEval 2019 Task 5 about the detection of hate speech against immigrants and women in Spanish and English messages extracted from Twitter. The task is organized in two related classification subtasks: a main binary subtask for detecting the presence of hate speech, and a finer-grained one devoted to identifying further features in hateful contents such as the aggressive attitude and the target harassed, to distinguish if the incitement is against an individual rather than a group. HatEval has been one of the most popular tasks in SemEval-2019 with a total of 108 submitted runs for Subtask A and 70 runs for Subtask B, from a total of 74 different teams. Data provided for the task are described by showing how they have been collected and annotated. Moreover, the paper provides an analysis and discussion about the participant systems and the results they achieved in both subtasks.",
}

@inproceedings{sanguinetti2018italian,
    title = "An {I}talian Twitter Corpus of Hate Speech against Immigrants",
    author = "Sanguinetti, Manuela  and
      Poletto, Fabio  and
      Bosco, Cristina  and
      Patti, Viviana  and
      Stranisci, Marco",
    booktitle = "Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC}-2018)",
    month = may,
    year = "2018",
    address = "Miyazaki, Japan",
    publisher = "European Languages Resources Association (ELRA)",
    url = "https://www.aclweb.org/anthology/L18-1443",
}

@inproceedings{ptaszynskiresults,
  title={Results of the PolEval 2019 Shared Task 6: First Dataset and Open Shared Task for Automatic Cyberbullying Detection in Polish Twitter},
  author={Ptaszynski, Michal and Pieciukiewicz, Agata and Dyba{\l}a, Pawe{\l}},
  booktitle={Proceedings of the PolEval 2019 Workshop},
  pages={89--110},
  url={http://poleval.pl/files/poleval2019.pdf#page=89},
  year={2019},
  publisher={Instytut Podstaw Informatyki PAN},
  address={Warszawa, Poland},
  isbn={978-83-63159-28-3}
}

@inproceedings{fortuna2019hierarchically,
    title = "A Hierarchically-Labeled {P}ortuguese Hate Speech Dataset",
    author = "Fortuna, Paula  and
      Rocha da Silva, Jo{\~a}o  and
      Soler-Company, Juan  and
      Wanner, Leo  and
      Nunes, S{\'e}rgio",
    booktitle = "Proceedings of the Third Workshop on Abusive Language Online",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-3510",
    doi = "10.18653/v1/W19-3510",
    pages = "94--104",
}

@article{broniatowski2020covid,
  title={The covid-19 social media infodemic reflects uncertainty and state-sponsored propaganda},
  author={Broniatowski, David A and Kerchner, Daniel and Farooq, Fouzia and Huang, Xiaolei and Jamison, Amelia M and Dredze, Mark and Quinn, Sandra Crouse},
  journal={arXiv preprint arXiv:2007.09682},
  year={2020}
}

@misc{huang2020coronavirus,
  title={Coronavirus Twitter Data: A collection of COVID-19 tweets with automated annotations},
  author={Huang, Xiaolei and Jamison, Amelia and Broniatowski, David and Quinn, Sandra and Dredze, Mark},
  year={2020}
}